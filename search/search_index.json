{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"StackHPC Release Train \u00b6 The StackHPC release train is a mechanism to provide tested, reproducible software releases for our supported clients. This site provides documentation for the StackHPC release train, covering this and other related repositories, as well as the services that comprise the live system. Background \u00b6 The CentOS 8 Stream announcement has caused us to do something we knew we wanted to do anyway (but had never got around to). We must provide a method for tested and reproducible releases for our supported clients. A related goal is to minimise unnecessary divergence between configuration of client deployments. Aim \u00b6 The ultimate aim of the release train is to provide tested and reproducible software releases. This includes artifacts, metadata, and the configuration necessary to consume them. Scope \u00b6 The current scope of the release train covers OpenStack control plane deployment, and all the dependencies thereof. This includes: Host Operating System (OS) repositories, e.g. Rocky Linux 9 BaseOS Source code repositories Kolla container images Ironic Python Agent (IPA) deployment images VM & bare metal disk images In future, the scope may expand to cover other software, such as Kubernetes. Configuration \u00b6 StackHPC provides a base Kayobe configuration which includes settings required to consume the release train. Clients merge this configuration into their own, and apply site and environment-specific changes. Hosting and accessing content \u00b6 The release train artifacts are hosted via Pulp at https://ark.stackhpc.com . Access to the API and artifacts is controlled via client user account passwords. Clients deploy a local Pulp service which syncs with Ark. Automation & Continuous Integration (CI) \u00b6 Automation and CI are key aspects of the release train. The additional control provided by the release train comes at a cost in maintenance and complexity, which must be offset via automation and CI. Leveraging technologies such as; Ansible and Terraform in addition to services such as Github Workflows allows us to achieve the goals of the StackHPC release train. There are numerous applications of these aforementioned technologies and services across various repositories within the StackHPC organisation, they are as follows;","title":"Overview"},{"location":"#stackhpc-release-train","text":"The StackHPC release train is a mechanism to provide tested, reproducible software releases for our supported clients. This site provides documentation for the StackHPC release train, covering this and other related repositories, as well as the services that comprise the live system.","title":"StackHPC Release Train"},{"location":"#background","text":"The CentOS 8 Stream announcement has caused us to do something we knew we wanted to do anyway (but had never got around to). We must provide a method for tested and reproducible releases for our supported clients. A related goal is to minimise unnecessary divergence between configuration of client deployments.","title":"Background"},{"location":"#aim","text":"The ultimate aim of the release train is to provide tested and reproducible software releases. This includes artifacts, metadata, and the configuration necessary to consume them.","title":"Aim"},{"location":"#scope","text":"The current scope of the release train covers OpenStack control plane deployment, and all the dependencies thereof. This includes: Host Operating System (OS) repositories, e.g. Rocky Linux 9 BaseOS Source code repositories Kolla container images Ironic Python Agent (IPA) deployment images VM & bare metal disk images In future, the scope may expand to cover other software, such as Kubernetes.","title":"Scope"},{"location":"#configuration","text":"StackHPC provides a base Kayobe configuration which includes settings required to consume the release train. Clients merge this configuration into their own, and apply site and environment-specific changes.","title":"Configuration"},{"location":"#hosting-and-accessing-content","text":"The release train artifacts are hosted via Pulp at https://ark.stackhpc.com . Access to the API and artifacts is controlled via client user account passwords. Clients deploy a local Pulp service which syncs with Ark.","title":"Hosting and accessing content"},{"location":"#automation-continuous-integration-ci","text":"Automation and CI are key aspects of the release train. The additional control provided by the release train comes at a cost in maintenance and complexity, which must be offset via automation and CI. Leveraging technologies such as; Ansible and Terraform in addition to services such as Github Workflows allows us to achieve the goals of the StackHPC release train. There are numerous applications of these aforementioned technologies and services across various repositories within the StackHPC organisation, they are as follows;","title":"Automation &amp; Continuous Integration (CI)"},{"location":"contributing/","text":"Contributing \u00b6 Modifying the documentation \u00b6 This documentation is built via MkDocs and hosted via GitHub pages . The configuration file is mkdocs.yml , and documentation Markdown source is in docs/ . GitHub Actions workflows build the documentation in pull requests, and deploy it to GitHub pages on pushes to main . To build and serve the documentation locally at http://127.0.0.1:8000/stackhpc-release-train : python3 -m venv mkdocs-venv source mkdocs-venv/bin/activate pip install -U pip pip install -r docs-requirements.txt mkdocs serve","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#modifying-the-documentation","text":"This documentation is built via MkDocs and hosted via GitHub pages . The configuration file is mkdocs.yml , and documentation Markdown source is in docs/ . GitHub Actions workflows build the documentation in pull requests, and deploy it to GitHub pages on pushes to main . To build and serve the documentation locally at http://127.0.0.1:8000/stackhpc-release-train : python3 -m venv mkdocs-venv source mkdocs-venv/bin/activate pip install -U pip pip install -r docs-requirements.txt mkdocs serve","title":"Modifying the documentation"},{"location":"architecture/","text":"Architecture \u00b6 This page covers architecture of the StackHPC release train. It assumes familiarity with the overview . See the original release train design document for design & requirements. Components \u00b6 The following diagram shows the major components of the release train. This diagram was created using Google Drawings . Pulps \u00b6 Pulp is a content server that manages repositories of software packages and facilitates their distribution to content consumers. The core functionality of Pulp is to provide versioned collections of software packages for onward distribution to consumers. Pulp's ability to host multiple snapshots of a repository make it a good choice for hosting the release train content. There are three types of Pulp service in the release train architecture. Ark is a public-facing production Pulp service hosted on a public cloud provider, leaf.cloud . It is the master copy of development and released content, and makes the content available to clients. Clients access Ark via a Pulp service deployed on their local infrastructure. Content is synced from Ark to the local Pulp service, and control plane hosts acquire the content from there. Test is an internal service running on the SMS lab cloud. Content is synced from Ark to test , providing a local content mirror for testing in SMS lab. In some respects, the test Pulp service may be considered a client. Content \u00b6 Various different types of content are hosted by Pulp, including: RPM package repositories ( Pulp RPM plugin ) Rocky Linux distribution packages Third party packages Apt package repositories ( Pulp Deb plugin ) Ubuntu distribution packages Third party packages Container image repositories ( Pulp container plugin ) Kolla container images File repositories ( Pulp file plugin ) Disk images Some of this content may be mirrored from upstream sources, while others are the result of release train build processes. Access control \u00b6 Access to released Pulp content is restricted to clients with a support agreement. Build and test processes also need access to content. Package repositories \u00b6 Access to package repositories is controlled via Pulp RBAC content guards. Two content guards are in use - development and release . The development content guard is assigned to unreleased content, while the release content guard is assigned to released content. Clients are provided with a username and password which they use when syncing package repositories in their local Pulp service with Ark. Clients' credentials are authorised to access content protected by the release content guard. Build and test processes are provided with a user account that is authorised to access the development and release content guard. Containers \u00b6 Access to container images is controlled by token authentication, which uses Django users in the backend. Two container namespaces are in use - stackhpc-dev and stackhpc . The stackhpc-dev namespace is used for unreleased content, while the stackhpc namespace is used for released content. Clients are provided with a set of credentials, which they use when syncing container image repositories in their local Pulp service with Ark. Clients' credentials are authorised to pull from the stackhpc namespace. Build and test processes are provided with credentials that are authorised to push to the stackhpc-dev namespace. Syncing package repositories \u00b6 At the top of the diagram above are the upstream sources. Some of these may be mirrored/synced into Ark, including: OS distribution package repositories, e.g. Rocky Linux 9 BaseOS Third party package repositories, e.g. Grafana The Sync package repositories GitHub Actions workflow runs nightly and on demand, ensuring that we have regular versioned snapshots of these repositories. Synced content is immediately published and distributed, such that it is available to build & test processes. After a successful sync in Ark, the content is synced to the test Pulp service. Mirrored content typically uses a policy of immediate , meaning that all content is downloaded from the upstream source during the sync. This avoids issues seen with the on_demand policy where content that is removed from the upstream source becomes inaccessible if it has not been previously requested by a client. For RPM content, we also use a sync_policy of mirror_complete , which removes content from the snapshots in line with upstream repositories (in contrast with the default additive sync_policy , which does not). There are a couple of repositories for which mirror_complete does not work, so we use mirror_content_only instead. Versioning \u00b6 Package repositories are versioned based on the date/time stamp at the beginning of the sync workflow, e.g. 20211122T102435 . This version string is used as the final component of the path at which the corresponding distribution is hosted. For example, a Rocky Linux 9 BaseOS snapshot may be hosted at https://ark.stackhpc.com/pulp/content/rocky/9/BaseOS/x86_64/os/20240105T044843/. The rationale behind using a date/time stamp is that there is no sane way to version a large collection of content, such as a repository, in a way in which the version reflects changes in the content (e.g. SemVer). While the timestamp used is fairly arbitrary, it does at least provide a reasonable guarantee of ordering, and is easily automated. Building \u00b6 Build processes may take as input the synced repository mirrors and other external sources, including: Python Package Index (PyPI) StackHPC source code repositories Other source code repositories e.g. GitHub, OpenDev The outputs of these build processes are pushed to Ark. Build and test processes run on SMS cloud, to avoid excessive running costs. All content in Ark that is required by the build and test processes is synced to the test Pulp service running in SMS cloud, minimising data egress from Ark. Kolla container images \u00b6 Kolla container images are built via Kayobe, using a ci-builder environment in StackHPC Kayobe config . The configuration uses the package repositories in Ark when building containers. Images are built using a manually triggered GitHub Actions workflow . The stackhpc-dev namespace in Ark contains container push repositories , which are pushed to using Kayobe. The Sync container repositories GitHub Actions workflow runs demand, syncing container repositories in test Pulp service with those in Ark. It also configures container image distributions to be private, since they are public by default. Kolla container images are versioned based on the OpenStack release name, OS distribution and the date/time stamp at the beginning of the build workflow, e.g. 2024.1-rocky-9-20240922T102435 . This version string is used as the image tag. Unlike package repositories, container image tags allow multiple versions to be present in a distribution of a container repository simultaneously. We therefore use separate namespaces for development ( stackhpc-dev ) and release ( stackhpc ). Disk images \u00b6 Overcloud host images are built via Kayobe, using the same ci-builder environment used to build Kolla container images. Overcloud images are built using a manually triggered GitHub Actions workflow . They are pushed to a Pulp file repository in Ark, and uploaded to the SMS lab Glance image service for all-in-one and multi-node testing. Overcloud images are versioned based on the OpenStack release name, and the date/time stamp at the beginning of the build workflow, e.g. 2024.1-20240922T102435 . This version string is included in the Pulp distribution base_path of the image, e.g. https://ark.stackhpc.com/pulp/content/kayobe-images/2024.1/rocky/9/2024.1-20240922T102435/overcloud-rocky-9.qcow2 Testing \u00b6 Release train content is tested via a Kayobe deployment of OpenStack. A ci-aio environment in StackHPC Kayobe config provides a converged control/compute host for testing. Various all-in-one OpenStack test configurations are run against pull requests opened against the StackHPC Kayobe config repository. Promotion \u00b6 Whether content is mirrored from an upstream source or built locally, it is not immediately released. Promotion describes the process whereby release candidate content is made into a release that is available to clients. For package repositories and overcloud host images, promotion does not affect how content is accessed, only who may access it. Promotion involves changing the content guard for the distribution to be released from development to release . This makes the content accessible to clients using their client credentials. The stackhpc container namespace contains regular container repositories, which cannot be pushed to via docker push . Instead, we use the Pulp API to sync specific tags from stackhpc-dev to stackhpc . Client Pulp service \u00b6 Clients access Ark via a Pulp service deployed on their local infrastructure. Typically the Pulp service is deployed as a Pulp in one container running on the seed host. Content is synced from Ark to the local Pulp service, and control plane hosts acquire the content from there. This avoids excessive Internet bandwidth usage, both for the client and Ark. Content in the client Pulp service is synced using the on_demand policy. This avoids unnecessarily large storage requirements on the seed, and speeds up syncing. There should be no risk of content becoming permanently unavailable, so long as Ark continues to host sufficiently old versions. This approach does have a downside of requiring Ark to be available to provide any content which has not previously been downloaded. Client configuration \u00b6 In order to consume the release train, clients should migrate to StackHPC Kayobe config . This repository provides configuration and playbooks to: deploy a local Pulp service as a container on the seed package repository versions to use container image tags to use sync all necessary content from Ark into the local Pulp service use the local Pulp repository mirrors on control plane hosts use the local Pulp container registry on control plane hosts This configuration is in active development and is expected to evolve over the coming releases. Further documentation of this configuration is out of scope here, but is available in the readme . Continuous Integration (CI) and automation \u00b6 The intention is to have as much as possible of the release train automated and run via CI. Typically, workflows may go through the following stages as they evolve: automated via Ansible, manually executed executed by GitHub Actions workflows, manually triggered by workflow dispatch or schedule executed by GitHub Actions workflows, automatically triggered by an event e.g. pull request or another workflow This sequence discourages putting too much automation into the GitHub Actions workflows, ensuring it is possible to run them manually. The release train Ansible playbooks make heavy use of the stackhpc.pulp collection, which in turn uses modules from the pulp.squeezer collection. Source Code Repositories \u00b6 To maintain the various source code repositories used within the Release Train there exists a number of automated processes. A mixture of Github workflows and Ansible playbooks are used to achieve this Github Workflows \u00b6 Upstream Sync : a number of repositories that are used by StackHPC are forks and therefore need to be synchronised with upstream to remain up-to-date. Therefore, this workflow will once a week make a pull request against any of the active openstack releases branches that are ahead of our downstream branch. Tox : in order to ensure that commits to the repositories are correct and follow style guidelines we can utilise tox which can automate the unit testing and linting of the codebase. This workflow will run anytime a push is made or a pull request to one of the active release branches. Tag & Release : various software and packages depend on the repositories therefore it is important that tags are made and releases are published. Ansible collection linters : linters and sanity tests for Ansible collections. Ansible \u00b6 Repository Synchronisation : as the workflows need to be located within each repository and branch that we wish to run them on. Therefore, the deployment and future updates are achieve through automation via Ansible. This allows for changes and new workflows to be automatically propagated out across the StackHPC organisation. This also manages the deployment of community files such as CODEOWNERS which can be used to automatically assign the relevant individuals to a newly opened pull request.","title":"Architecture"},{"location":"architecture/#architecture","text":"This page covers architecture of the StackHPC release train. It assumes familiarity with the overview . See the original release train design document for design & requirements.","title":"Architecture"},{"location":"architecture/#components","text":"The following diagram shows the major components of the release train. This diagram was created using Google Drawings .","title":"Components"},{"location":"architecture/#pulps","text":"Pulp is a content server that manages repositories of software packages and facilitates their distribution to content consumers. The core functionality of Pulp is to provide versioned collections of software packages for onward distribution to consumers. Pulp's ability to host multiple snapshots of a repository make it a good choice for hosting the release train content. There are three types of Pulp service in the release train architecture. Ark is a public-facing production Pulp service hosted on a public cloud provider, leaf.cloud . It is the master copy of development and released content, and makes the content available to clients. Clients access Ark via a Pulp service deployed on their local infrastructure. Content is synced from Ark to the local Pulp service, and control plane hosts acquire the content from there. Test is an internal service running on the SMS lab cloud. Content is synced from Ark to test , providing a local content mirror for testing in SMS lab. In some respects, the test Pulp service may be considered a client.","title":"Pulps"},{"location":"architecture/#content","text":"Various different types of content are hosted by Pulp, including: RPM package repositories ( Pulp RPM plugin ) Rocky Linux distribution packages Third party packages Apt package repositories ( Pulp Deb plugin ) Ubuntu distribution packages Third party packages Container image repositories ( Pulp container plugin ) Kolla container images File repositories ( Pulp file plugin ) Disk images Some of this content may be mirrored from upstream sources, while others are the result of release train build processes.","title":"Content"},{"location":"architecture/#access-control","text":"Access to released Pulp content is restricted to clients with a support agreement. Build and test processes also need access to content.","title":"Access control"},{"location":"architecture/#package-repositories","text":"Access to package repositories is controlled via Pulp RBAC content guards. Two content guards are in use - development and release . The development content guard is assigned to unreleased content, while the release content guard is assigned to released content. Clients are provided with a username and password which they use when syncing package repositories in their local Pulp service with Ark. Clients' credentials are authorised to access content protected by the release content guard. Build and test processes are provided with a user account that is authorised to access the development and release content guard.","title":"Package repositories"},{"location":"architecture/#containers","text":"Access to container images is controlled by token authentication, which uses Django users in the backend. Two container namespaces are in use - stackhpc-dev and stackhpc . The stackhpc-dev namespace is used for unreleased content, while the stackhpc namespace is used for released content. Clients are provided with a set of credentials, which they use when syncing container image repositories in their local Pulp service with Ark. Clients' credentials are authorised to pull from the stackhpc namespace. Build and test processes are provided with credentials that are authorised to push to the stackhpc-dev namespace.","title":"Containers"},{"location":"architecture/#syncing-package-repositories","text":"At the top of the diagram above are the upstream sources. Some of these may be mirrored/synced into Ark, including: OS distribution package repositories, e.g. Rocky Linux 9 BaseOS Third party package repositories, e.g. Grafana The Sync package repositories GitHub Actions workflow runs nightly and on demand, ensuring that we have regular versioned snapshots of these repositories. Synced content is immediately published and distributed, such that it is available to build & test processes. After a successful sync in Ark, the content is synced to the test Pulp service. Mirrored content typically uses a policy of immediate , meaning that all content is downloaded from the upstream source during the sync. This avoids issues seen with the on_demand policy where content that is removed from the upstream source becomes inaccessible if it has not been previously requested by a client. For RPM content, we also use a sync_policy of mirror_complete , which removes content from the snapshots in line with upstream repositories (in contrast with the default additive sync_policy , which does not). There are a couple of repositories for which mirror_complete does not work, so we use mirror_content_only instead.","title":"Syncing package repositories"},{"location":"architecture/#versioning","text":"Package repositories are versioned based on the date/time stamp at the beginning of the sync workflow, e.g. 20211122T102435 . This version string is used as the final component of the path at which the corresponding distribution is hosted. For example, a Rocky Linux 9 BaseOS snapshot may be hosted at https://ark.stackhpc.com/pulp/content/rocky/9/BaseOS/x86_64/os/20240105T044843/. The rationale behind using a date/time stamp is that there is no sane way to version a large collection of content, such as a repository, in a way in which the version reflects changes in the content (e.g. SemVer). While the timestamp used is fairly arbitrary, it does at least provide a reasonable guarantee of ordering, and is easily automated.","title":"Versioning"},{"location":"architecture/#building","text":"Build processes may take as input the synced repository mirrors and other external sources, including: Python Package Index (PyPI) StackHPC source code repositories Other source code repositories e.g. GitHub, OpenDev The outputs of these build processes are pushed to Ark. Build and test processes run on SMS cloud, to avoid excessive running costs. All content in Ark that is required by the build and test processes is synced to the test Pulp service running in SMS cloud, minimising data egress from Ark.","title":"Building"},{"location":"architecture/#kolla-container-images","text":"Kolla container images are built via Kayobe, using a ci-builder environment in StackHPC Kayobe config . The configuration uses the package repositories in Ark when building containers. Images are built using a manually triggered GitHub Actions workflow . The stackhpc-dev namespace in Ark contains container push repositories , which are pushed to using Kayobe. The Sync container repositories GitHub Actions workflow runs demand, syncing container repositories in test Pulp service with those in Ark. It also configures container image distributions to be private, since they are public by default. Kolla container images are versioned based on the OpenStack release name, OS distribution and the date/time stamp at the beginning of the build workflow, e.g. 2024.1-rocky-9-20240922T102435 . This version string is used as the image tag. Unlike package repositories, container image tags allow multiple versions to be present in a distribution of a container repository simultaneously. We therefore use separate namespaces for development ( stackhpc-dev ) and release ( stackhpc ).","title":"Kolla container images"},{"location":"architecture/#disk-images","text":"Overcloud host images are built via Kayobe, using the same ci-builder environment used to build Kolla container images. Overcloud images are built using a manually triggered GitHub Actions workflow . They are pushed to a Pulp file repository in Ark, and uploaded to the SMS lab Glance image service for all-in-one and multi-node testing. Overcloud images are versioned based on the OpenStack release name, and the date/time stamp at the beginning of the build workflow, e.g. 2024.1-20240922T102435 . This version string is included in the Pulp distribution base_path of the image, e.g. https://ark.stackhpc.com/pulp/content/kayobe-images/2024.1/rocky/9/2024.1-20240922T102435/overcloud-rocky-9.qcow2","title":"Disk images"},{"location":"architecture/#testing","text":"Release train content is tested via a Kayobe deployment of OpenStack. A ci-aio environment in StackHPC Kayobe config provides a converged control/compute host for testing. Various all-in-one OpenStack test configurations are run against pull requests opened against the StackHPC Kayobe config repository.","title":"Testing"},{"location":"architecture/#promotion","text":"Whether content is mirrored from an upstream source or built locally, it is not immediately released. Promotion describes the process whereby release candidate content is made into a release that is available to clients. For package repositories and overcloud host images, promotion does not affect how content is accessed, only who may access it. Promotion involves changing the content guard for the distribution to be released from development to release . This makes the content accessible to clients using their client credentials. The stackhpc container namespace contains regular container repositories, which cannot be pushed to via docker push . Instead, we use the Pulp API to sync specific tags from stackhpc-dev to stackhpc .","title":"Promotion"},{"location":"architecture/#client-pulp-service","text":"Clients access Ark via a Pulp service deployed on their local infrastructure. Typically the Pulp service is deployed as a Pulp in one container running on the seed host. Content is synced from Ark to the local Pulp service, and control plane hosts acquire the content from there. This avoids excessive Internet bandwidth usage, both for the client and Ark. Content in the client Pulp service is synced using the on_demand policy. This avoids unnecessarily large storage requirements on the seed, and speeds up syncing. There should be no risk of content becoming permanently unavailable, so long as Ark continues to host sufficiently old versions. This approach does have a downside of requiring Ark to be available to provide any content which has not previously been downloaded.","title":"Client Pulp service"},{"location":"architecture/#client-configuration","text":"In order to consume the release train, clients should migrate to StackHPC Kayobe config . This repository provides configuration and playbooks to: deploy a local Pulp service as a container on the seed package repository versions to use container image tags to use sync all necessary content from Ark into the local Pulp service use the local Pulp repository mirrors on control plane hosts use the local Pulp container registry on control plane hosts This configuration is in active development and is expected to evolve over the coming releases. Further documentation of this configuration is out of scope here, but is available in the readme .","title":"Client configuration"},{"location":"architecture/#continuous-integration-ci-and-automation","text":"The intention is to have as much as possible of the release train automated and run via CI. Typically, workflows may go through the following stages as they evolve: automated via Ansible, manually executed executed by GitHub Actions workflows, manually triggered by workflow dispatch or schedule executed by GitHub Actions workflows, automatically triggered by an event e.g. pull request or another workflow This sequence discourages putting too much automation into the GitHub Actions workflows, ensuring it is possible to run them manually. The release train Ansible playbooks make heavy use of the stackhpc.pulp collection, which in turn uses modules from the pulp.squeezer collection.","title":"Continuous Integration (CI) and automation"},{"location":"architecture/#source-code-repositories","text":"To maintain the various source code repositories used within the Release Train there exists a number of automated processes. A mixture of Github workflows and Ansible playbooks are used to achieve this","title":"Source Code Repositories"},{"location":"architecture/#github-workflows","text":"Upstream Sync : a number of repositories that are used by StackHPC are forks and therefore need to be synchronised with upstream to remain up-to-date. Therefore, this workflow will once a week make a pull request against any of the active openstack releases branches that are ahead of our downstream branch. Tox : in order to ensure that commits to the repositories are correct and follow style guidelines we can utilise tox which can automate the unit testing and linting of the codebase. This workflow will run anytime a push is made or a pull request to one of the active release branches. Tag & Release : various software and packages depend on the repositories therefore it is important that tags are made and releases are published. Ansible collection linters : linters and sanity tests for Ansible collections.","title":"Github Workflows"},{"location":"architecture/#ansible","text":"Repository Synchronisation : as the workflows need to be located within each repository and branch that we wish to run them on. Therefore, the deployment and future updates are achieve through automation via Ansible. This allows for changes and new workflows to be automatically propagated out across the StackHPC organisation. This also manages the deployment of community files such as CODEOWNERS which can be used to automatically assign the relevant individuals to a newly opened pull request.","title":"Ansible"},{"location":"operations/ark/","text":"Ark \u00b6 Ark is deployed on a single host that runs on Leafcloud. Data is stored in Leafcloud's object store. An Ansible playbook automates deployment of Pulp and its dependencies using the Pulp OCI images and Compose configuration provided by the Pulp project.","title":"Ark"},{"location":"operations/ark/#ark","text":"Ark is deployed on a single host that runs on Leafcloud. Data is stored in Leafcloud's object store. An Ansible playbook automates deployment of Pulp and its dependencies using the Pulp OCI images and Compose configuration provided by the Pulp project.","title":"Ark"},{"location":"operations/github/","text":"Operations - GitHub \u00b6 GitHub Actions Runner Controller (ARC) \u00b6 Some GitHub Actions workflows run on public runners, while others require access to specific services or benefit from data locality. In the latter case we use GitHub's Actions Runner Controller (ARC) (not to be confused with our Ark...) to provide dynamically scaling containerised private GitHub Actions runners. The ARC-Installer repository contains scripts and Helm values.yaml configuration for registering the ARC controller services and runner scale sets. Any project that wishes to use runners on this cluster should define its runner scale set configuration in the ARC-Installer repository. [LEGACY] GitHub Actions runners \u00b6 We are no longer using this approach, but it is retained in case it is useful in future. This Terraform configuration deploys a GitHub Actions runner VMs on an OpenStack cloud for the stackhpc-release-train repository. Usage \u00b6 These instructions show how to use this Terraform configuration manually. They assume you are running an Ubuntu host that will be used to run Terraform. The machine should have network access to the VM that will be created by this configuration. Install Terraform: wget -qO - terraform.gpg https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/terraform-archive-keyring.gpg sudo echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/terraform-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/terraform.list sudo apt update sudo apt install terraform Clone and initialise the repo: git clone https://github.com/stackhpc/stackhpc-release-train cd stackhpc-release-train Change to the terraform/github-runners directory: cd terraform/github-runners Initialise Terraform: terraform init Create an OpenStack clouds.yaml file with your credentials to access an OpenStack cloud. Alternatively, download one from Horizon. The credentials should be scoped to the stackhpc-release project. cat << EOF > clouds.yaml --- clouds: sms-lab: auth: auth_url: https://api.sms-lab.cloud:5000 username: <username> project_name: stackhpc-release domain_name: default interface: public EOF Export environment variables to use the correct cloud and provide a password: export OS_CLOUD=sms-lab read -p OS_PASSWORD -s OS_PASSWORD export OS_PASSWORD Verify that the Terraform variables in terraform.tfvars are correct. Generate a plan: terraform plan Apply the changes: terraform apply -auto-approve Create a virtualenv: python3 -m venv venv Activate the virtualenv: source venv/bin/activate Install Python dependencies: pip install -r ansible/requirements.txt Install Ansible Galaxy dependencies: ansible-galaxy collection install -r ansible/requirements.yml ansible-galaxy role install -r ansible/requirements.yml Create a GitHub PAT token (classic) with repo:all scope. Export an environment variable with the token. read -p PERSONAL_ACCESS_TOKEN -s PERSONAL_ACCESS_TOKEN export PERSONAL_ACCESS_TOKEN Deploy runners: ansible-playbook ansible/site.yml -i ansible/inventory.yml To remove runners: ansible-playbook ansible/site.yml -i ansible/inventory.yml -e runner_state=absent Troubleshooting \u00b6 Install service fails \u00b6 If you see the following: TASK [monolithprojects.github_actions_runner : Install service] ******************************************************************************************************************************************** fatal: [10.205.0.50]: FAILED! => changed=true cmd: ./svc.sh install ubuntu msg: '[Errno 2] No such file or directory: b''./svc.sh''' rc: 2 stderr: '' stderr_lines: <omitted> stdout: '' stdout_lines: <omitted> It might mean the runner is already registered, possibly from a previous VM. Remove the runner using Ansible or the GitHub settings.","title":"GitHub"},{"location":"operations/github/#operations-github","text":"","title":"Operations - GitHub"},{"location":"operations/github/#github-actions-runner-controller-arc","text":"Some GitHub Actions workflows run on public runners, while others require access to specific services or benefit from data locality. In the latter case we use GitHub's Actions Runner Controller (ARC) (not to be confused with our Ark...) to provide dynamically scaling containerised private GitHub Actions runners. The ARC-Installer repository contains scripts and Helm values.yaml configuration for registering the ARC controller services and runner scale sets. Any project that wishes to use runners on this cluster should define its runner scale set configuration in the ARC-Installer repository.","title":"GitHub Actions Runner Controller (ARC)"},{"location":"operations/github/#legacy-github-actions-runners","text":"We are no longer using this approach, but it is retained in case it is useful in future. This Terraform configuration deploys a GitHub Actions runner VMs on an OpenStack cloud for the stackhpc-release-train repository.","title":"[LEGACY] GitHub Actions runners"},{"location":"operations/github/#usage","text":"These instructions show how to use this Terraform configuration manually. They assume you are running an Ubuntu host that will be used to run Terraform. The machine should have network access to the VM that will be created by this configuration. Install Terraform: wget -qO - terraform.gpg https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/terraform-archive-keyring.gpg sudo echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/terraform-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/terraform.list sudo apt update sudo apt install terraform Clone and initialise the repo: git clone https://github.com/stackhpc/stackhpc-release-train cd stackhpc-release-train Change to the terraform/github-runners directory: cd terraform/github-runners Initialise Terraform: terraform init Create an OpenStack clouds.yaml file with your credentials to access an OpenStack cloud. Alternatively, download one from Horizon. The credentials should be scoped to the stackhpc-release project. cat << EOF > clouds.yaml --- clouds: sms-lab: auth: auth_url: https://api.sms-lab.cloud:5000 username: <username> project_name: stackhpc-release domain_name: default interface: public EOF Export environment variables to use the correct cloud and provide a password: export OS_CLOUD=sms-lab read -p OS_PASSWORD -s OS_PASSWORD export OS_PASSWORD Verify that the Terraform variables in terraform.tfvars are correct. Generate a plan: terraform plan Apply the changes: terraform apply -auto-approve Create a virtualenv: python3 -m venv venv Activate the virtualenv: source venv/bin/activate Install Python dependencies: pip install -r ansible/requirements.txt Install Ansible Galaxy dependencies: ansible-galaxy collection install -r ansible/requirements.yml ansible-galaxy role install -r ansible/requirements.yml Create a GitHub PAT token (classic) with repo:all scope. Export an environment variable with the token. read -p PERSONAL_ACCESS_TOKEN -s PERSONAL_ACCESS_TOKEN export PERSONAL_ACCESS_TOKEN Deploy runners: ansible-playbook ansible/site.yml -i ansible/inventory.yml To remove runners: ansible-playbook ansible/site.yml -i ansible/inventory.yml -e runner_state=absent","title":"Usage"},{"location":"operations/github/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"operations/github/#install-service-fails","text":"If you see the following: TASK [monolithprojects.github_actions_runner : Install service] ******************************************************************************************************************************************** fatal: [10.205.0.50]: FAILED! => changed=true cmd: ./svc.sh install ubuntu msg: '[Errno 2] No such file or directory: b''./svc.sh''' rc: 2 stderr: '' stderr_lines: <omitted> stdout: '' stdout_lines: <omitted> It might mean the runner is already registered, possibly from a previous VM. Remove the runner using Ansible or the GitHub settings.","title":"Install service fails"},{"location":"operations/test-pulp/","text":"Test Pulp is a single bare metal pulp-server instance in SMS Lab's stackhpc-release project. It can be reached via SMS Lab's bastion host: ssh centos@10.205.3.187 -J 185.45.78.150 If your SSH key is not there, ask for access on #release-train Slack channel. It utilises an all-in-one Pulp container run via podman: podman run --name pulp \\ --volume /etc/pulp:/etc/pulp \\ -p 8080:80 \\ -p 24817:24817 \\ --volume pulp_storage:/var/lib/pulp \\ --volume pulp_pgsql:/var/lib/pgsql \\ --volume pulp_containers:/var/lib/containers \\ --shm-size=1g \\ pulp/pulp CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3ab31fcb4221 docker.io/pulp/pulp:latest 5 months ago Up 16 minutes ago 0.0.0.0:8080->80/tcp, 0.0.0.0:24817->24817/tcp pulp Pretty basic setup done manually, with the following /etc/pulp/settings.py config file: CONTENT_ORIGIN='http://10.205.3.187:8080' ANSIBLE_API_HOSTNAME='http://10.205.3.187:8080' ANSIBLE_CONTENT_HOSTNAME='http://10.205.3.187:8080/pulp/content' TOKEN_AUTH_DISABLED=True TOKEN_SERVER='http://10.205.3.187:24817/token' ALLOWED_CONTENT_CHECKSUMS = [\"sha1\", \"sha224\", \"sha256\", \"sha384\", \"sha512\"]","title":"Test Pulp"},{"location":"usage/access/","text":"Usage - access control \u00b6 Access control is primarily managed using the Ansible playbooks and configuration in the stackhpc-release-train and stackhpc-release-train-clients repositories. The readme file in each repository provides information on how to install dependencies and run playbooks. The playbooks are designed to be run manually or via a GitHub Actions CI job. This page covers the different workflows available for access control. Create content guards \u00b6 This workflow must currently be run manually. It involves the following playbook in the stackhpc-release-train repository: dev-pulp-content-guards.yml : Create RBAC content-guards. It should almost never need to be run. To run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-content-guards.yml Configuration for content guards is in ansible/inventory/group_vars/all/dev-pulp-repos . Generating client credentials \u00b6 Clients require credentials in order to access Ark. See the README in the stackhpc-release-train-clients repository for how to generate and update them. It is also possible to use this repository to generate credentials with access to the unpromoted \"development\" content, but these should not be given to clients.","title":"Access control"},{"location":"usage/access/#usage-access-control","text":"Access control is primarily managed using the Ansible playbooks and configuration in the stackhpc-release-train and stackhpc-release-train-clients repositories. The readme file in each repository provides information on how to install dependencies and run playbooks. The playbooks are designed to be run manually or via a GitHub Actions CI job. This page covers the different workflows available for access control.","title":"Usage - access control"},{"location":"usage/access/#create-content-guards","text":"This workflow must currently be run manually. It involves the following playbook in the stackhpc-release-train repository: dev-pulp-content-guards.yml : Create RBAC content-guards. It should almost never need to be run. To run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-content-guards.yml Configuration for content guards is in ansible/inventory/group_vars/all/dev-pulp-repos .","title":"Create content guards"},{"location":"usage/access/#generating-client-credentials","text":"Clients require credentials in order to access Ark. See the README in the stackhpc-release-train-clients repository for how to generate and update them. It is also possible to use this repository to generate credentials with access to the unpromoted \"development\" content, but these should not be given to clients.","title":"Generating client credentials"},{"location":"usage/content-howto/","text":"Usage - content management HOWTO \u00b6 While the content management workflows documentation provides information on each of the workflows provided by the release train, it does not give instructions for how to compose them to achieve common tasks. That is the aim of this page. Update package repositories \u00b6 Update one or more package repositories to a new version, then build new Kolla container images from those repositories. If using Yoga release or earlier: Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images If using Zed release onwards: Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images (happens automatically) Add a new package repository \u00b6 Add one or more new package repositories to the release train, then add them to the Kayobe configuration and build new Kolla container images from those repositories. If using Yoga release or earlier: Add new package repositories to Release Train Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Add package repositories to StackHPC Kayobe Configuration Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images If using Zed release onwards: Add new package repositories to Release Train Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Add package repositories to StackHPC Kayobe Configuration Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images (happens automatically) Update Kolla container images \u00b6 Update one or more Kolla container images, without updating package repositories. If using Yoga release or earlier: Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images If using Zed release onwards: Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images (happens automatically) Add a new Kolla container image \u00b6 Set up builds for the image \u00b6 The list of services supported by StackHPC Kayobe configuration is defined via the feature flags in the ci-builder environment. To add a new service, add the relevant feature flag (see etc/kayobe/kolla.yml for supported flags). For example: kolla_enable_foo : true Create a PR for the change. Set up Test Pulp syncing for the image \u00b6 Next, the new images must be added to the kolla_container_images list in stackhpc-release-train . For example: kolla_container_images : 8< - foo-api - foo-manager 8< Create a PR for the change. Build and consume the image \u00b6 Once the two PRs have been approved and merged, follow the steps above to build and consume the new images. Set up client Pulp syncing for the image \u00b6 Finally, the new images must be added to the stackhpc_pulp_images list in etc/kayobe/pulp.yml . This updates the list of images that are synced from Ark to clients' local Pulp service. This step should be performed last, once the images have been pushed to Ark and promoted, otherwise client container syncs would fail. stackhpc_pulp_images : 8< - foo-api - foo-manager 8< Create a PR for the change.","title":"Content management HOWTO"},{"location":"usage/content-howto/#usage-content-management-howto","text":"While the content management workflows documentation provides information on each of the workflows provided by the release train, it does not give instructions for how to compose them to achieve common tasks. That is the aim of this page.","title":"Usage - content management HOWTO"},{"location":"usage/content-howto/#update-package-repositories","text":"Update one or more package repositories to a new version, then build new Kolla container images from those repositories. If using Yoga release or earlier: Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images If using Zed release onwards: Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images (happens automatically)","title":"Update package repositories"},{"location":"usage/content-howto/#add-a-new-package-repository","text":"Add one or more new package repositories to the release train, then add them to the Kayobe configuration and build new Kolla container images from those repositories. If using Yoga release or earlier: Add new package repositories to Release Train Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Add package repositories to StackHPC Kayobe Configuration Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images If using Zed release onwards: Add new package repositories to Release Train Sync package repositories (optional: runs nightly as a scheduled GitHub Action) Add package repositories to StackHPC Kayobe Configuration Update Kayobe repository versions Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images (happens automatically)","title":"Add a new package repository"},{"location":"usage/content-howto/#update-kolla-container-images","text":"Update one or more Kolla container images, without updating package repositories. If using Yoga release or earlier: Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images If using Zed release onwards: Build & push Kolla container images Update Kayobe container image tags Test Review & merge Kayobe configuration changes Promote container images (happens automatically)","title":"Update Kolla container images"},{"location":"usage/content-howto/#add-a-new-kolla-container-image","text":"","title":"Add a new Kolla container image"},{"location":"usage/content-howto/#set-up-builds-for-the-image","text":"The list of services supported by StackHPC Kayobe configuration is defined via the feature flags in the ci-builder environment. To add a new service, add the relevant feature flag (see etc/kayobe/kolla.yml for supported flags). For example: kolla_enable_foo : true Create a PR for the change.","title":"Set up builds for the image"},{"location":"usage/content-howto/#set-up-test-pulp-syncing-for-the-image","text":"Next, the new images must be added to the kolla_container_images list in stackhpc-release-train . For example: kolla_container_images : 8< - foo-api - foo-manager 8< Create a PR for the change.","title":"Set up Test Pulp syncing for the image"},{"location":"usage/content-howto/#build-and-consume-the-image","text":"Once the two PRs have been approved and merged, follow the steps above to build and consume the new images.","title":"Build and consume the image"},{"location":"usage/content-howto/#set-up-client-pulp-syncing-for-the-image","text":"Finally, the new images must be added to the stackhpc_pulp_images list in etc/kayobe/pulp.yml . This updates the list of images that are synced from Ark to clients' local Pulp service. This step should be performed last, once the images have been pushed to Ark and promoted, otherwise client container syncs would fail. stackhpc_pulp_images : 8< - foo-api - foo-manager 8< Create a PR for the change.","title":"Set up client Pulp syncing for the image"},{"location":"usage/content-workflows/","text":"Usage - content management workflows \u00b6 Content is primarily managed using the Ansible playbooks and configuration in the stackhpc-release-train repository. The readme provides information on how to install dependencies and run playbooks. The playbooks are designed to be run manually or via a GitHub Actions CI job. This page covers the different workflows available for content management. It may be necessary to combine multiple of these to achieve a desired outcome. Add new package repositories to Release Train \u00b6 Configuration for package repositories is in: ansible/inventory/group_vars/all/package-repos New package repositories should be added to rpm_package_repos or deb_package_repos in ansible/inventory/group_vars/all/package-repos . The format of these variables is defined in the same file. The following files contain information derived from the above variables and should not need to be modified: ansible/inventory/group_vars/all/dev-pulp-repos ansible/inventory/group_vars/all/test-pulp-repos Syncing package repositories \u00b6 The Sync package repositories workflow runs nightly and on demand. It syncs package repositories in Ark with their upstream sources, then creates publications and distributions for any new content: dev-pulp-repo-sync.yml : Synchronise ark with upstream package repositories. dev-pulp-repo-publication-cleanup.yml : Work around an issue with Pulp syncing where multiple publications may exist for a single repository version, breaking the Ansible Squeezer rpm_publication module. dev-pulp-repo-publish.yml : Create development distributions on ark for any new package repository snapshots. Next, it syncs new content in Ark to the test Pulp service: test-pulp-repo-version-query.yml : Query ark for the latest distribution versions and set the version map variable test_pulp_repository_rpm_repo_versions . test-pulp-repo-sync.yml : Synchronise test with ark 's package repositories using the version map variable test_pulp_repository_rpm_repo_versions . test-pulp-repo-publish.yml : Create distributions on test for any new package repository snapshots. It may be necessary to run this workflow on demand if the nightly workflow fails, or if an upstream package has been updated since the last run, or if the repository configuration has been updated. Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-repo-sync.yml \\ ansible/dev-pulp-repo-publication-cleanup.yml \\ ansible/dev-pulp-repo-publish.yml ansible-playbook -i ansible/inventory \\ ansible/test-pulp-repo-version-query.yml \\ ansible/test-pulp-repo-sync.yml \\ ansible/test-pulp-repo-publish.yml If a set of versions other than the latest need to be synced from Ark to test, then it is possible to specify test_pulp_repository_rpm_repo_versions via an extra variables file. In this case, it is not necessary to run test-pulp-repo-version-query.yml . For example: ansible-playbook -i ansible/inventory \\ ansible/test-pulp-repo-sync.yml \\ ansible/test-pulp-repo-publish.yml -e @test_pulp_repository_rpm_repo_versions.yml Here, test_pulp_repository_rpm_repo_versions.yml contains the repository version map variable test_pulp_repository_rpm_repo_versions . It maps package repository short names (in ansible/inventory/group_vars/all/package-repos ) to the version of that repository to sync and publish. For example: test_pulp_repository_rpm_repo_versions: centos_stream_8_appstream: 20211122T102435 centos_stream_8_baseos: 20220101T143229 ... Promoting package repositories \u00b6 Note This should only be performed when package repositories are ready for release. The Promote package repositories workflow is triggered automatically when a change is merged to stackhpc-kayobe-config. It may also be run on demand. It runs the following playbooks: dev-pulp-repo-version-query-kayobe.yml : Query the Pulp repository versions defined in a Kayobe configuration repository and sets the version map variable dev_pulp_distribution_rpm_promote_versions based upon those versions. A path to a Kayobe configuration repository must be specified via kayobe_config_repo_path . dev-pulp-repo-promote.yml : Promote the set of ark distributions defined in the version map variable dev_pulp_distribution_rpm_promote_versions to releases. Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-repo-version-query-kayobe.yml \\ ansible/dev-pulp-repo-promote.yml \\ -e kayobe_config_repo_path=../stackhpc-kayobe-config/ In this example, the Pulp repository versions defined in the etc/kayobe/pulp-repo-versions.yml file in ../stackhpc-kayobe-config repository (relative to the current working directory) will be promoted to releases. Alternatively, the set of versions to promote may be specified as an extra variables file: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-repo-promote.yml \\ -e @dev_pulp_distribution_rpm_promote_versions.yml Here, dev_pulp_distribution_rpm_promote_versions.yml contains the repository version map variable dev_pulp_distribution_rpm_promote_versions . It maps package repository short names (in ansible/inventory/group_vars/all/package-repos ) to the version of that repository to promote. For example: dev_pulp_distribution_rpm_promote_versions: centos_stream_8_appstream: 20211122T102435 centos_stream_8_baseos: 20220101T143229 ... Updating package repository versions in Kayobe configuration \u00b6 Note This procedure is expected to change. The Update Kayobe package repository versions workflow runs on demand. It should be run when package repository versions in Kayobe need to be updated. It runs the following playbooks: test-pulp-repo-version-query.yml : Query ark for the latest distribution versions and set the version map variable test_pulp_repository_rpm_repo_versions . test-kayobe-repo-version-generate.yml : Query stackhpc-kayobe-config for the current repository versions, then update them to the previously queried versions in ark . It then stores the new versions YAML file as an artifact, which may be downloaded and manually applied to stackhpc-kayobe-config. In future the workflow will be extended to create a PR. Use GitHub Actions to run this workflow, or to run it manually: ansible/test-pulp-repo-version-query.yml \\ ansible/test-kayobe-repo-version-generate.yml \\ -e kayobe_config_repo_path=./stackhpc-kayobe-config/ Package repository versions are stored in StackHPC Kayobe configuration in etc/kayobe/pulp-repo-versions.yml . Note that the updated versions are not necessarily released. The generated file may be amended as necessary (in case not all updates are required), then copied to the StackHPC Kayobe configuration. Adding package repositories in Kayobe configuration \u00b6 Adding a package repository to the StackHPC Release Train configuration is not sufficient to allow StackHPC OpenStack deployments to use it. The repository must also be defined in StackHPC Kayobe Configuration. We need to define how to sync the package repository from Ark into the local Pulp, as well as how control plane hosts access the repository in the local Pulp service. In the following steps, the short_name of a repository is the short_name field of the repository in ansible/inventory/group_vars/all/package-repos . Add details of the repository to stackhpc_pulp_rpm_repos or stackhpc_pulp_deb_repos in etc/kayobe/pulp.yml to enable syncing of the repository to the local Pulp service. Use the required field to avoid growing sync durations by controlling when the repository needs to be synced. Add a version variable to etc/kayobe/pulp.yml . It should have a format of stackhpc_pulp_repo_<short_name>_version . The version value may be specified or omitted. If omitted, it can be populated automatically using the Update Kayobe package repository versions workflow. Add local Pulp URL and version variables for the repository to etc/kayobe/stackhpc.yml . They should have a format of stackhpc_repo_<short_name>_url and stackhpc_repo_<short_name>_version . Override the repository version variable in stackhpc-ci.yml in the ci-aio , ci-builder and ci-multinode Kayobe environments to use the datestamped version. If the repository needs to be accessible to the host OS of control plane hosts, add it to etc/kayobe/dnf.yml or etc/kayobe/apt.yml . If the repository needs to be accessible to Kolla container images, add it to the Kolla image build configuration section in etc/kayobe/kolla.yml . Building container images \u00b6 Note This procedure is expected to change. The Build Kolla container images workflow in the stackhpc-kayobe-config repository runs on demand. It should be run when new Kolla container images are required. All images may be built, or a specific set of images. All successfully built images will be pushed to Ark, in the stackhpc-dev namespace. An Overcloud container images artifact will be visible on the summary page of a completed successful workflow run. This artifact contains a list of the built images. After a successful container image build workflow, another workflow is triggered to sync the images to the test Pulp. In the following example, the user specified a regular expression of ^magnum , matching all of the Magnum images, and the base image that they depend on. REPOSITORY TAG IMAGE ID CREATED SIZE ark.stackhpc.com/stackhpc-dev/magnum-api 2024.1-rocky-9-20240811T091848 32f2b9299194 6 minutes ago 1.29GB ark.stackhpc.com/stackhpc-dev/magnum-conductor 2024.1-rocky-9-20240811T091848 35e4c1cda1a8 7 minutes ago 1.14GB ark.stackhpc.com/stackhpc-dev/magnum-base 2024.1-rocky-9-20240811T091848 3bd5f3e50aa3 7 minutes ago 1.14GB ark.stackhpc.com/stackhpc-dev/openstack-base 2024.1-rocky-9-20240811T091848 bd02fa0ec1d6 7 minutes ago 991MB ark.stackhpc.com/stackhpc-dev/base 2024.1-rocky-9-20240811T091848 bd02fa0ec1d6 7 minutes ago 991MB In this example, the base , openstack-base and Magnum images have been tagged 2024.1-rocky-9-20240811T091848 . Instructions for building Kolla container images manually are provided in the StackHPC kayobe config documentation . Publishing container images \u00b6 The Publish container repositories workflow runs after a push to the main branch of the stackhpc-release-train repository, when relevant files have changed. It runs the following playbooks: dev-pulp-container-publish.yml : Configure access control for development and release container repositories and distributions on ark . Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-container-publish.yml Configuration for container images is in: ansible/inventory/group_vars/all/kolla . ansible/inventory/group_vars/all/dev-pulp-containers . ansible/inventory/group_vars/all/test-pulp-containers . New images should be added to kolla_container_images in ansible/inventory/group_vars/all/kolla . Syncing container images \u00b6 The Sync container repositories workflow runs after a successful container image build workflow run, or on demand. It runs the following playbooks: test-pulp-container-sync.yml : Synchronise test with container images from stackhpc-dev namespace on ark . test-pulp-container-publish.yml : Create distributions on test Pulp server for any new container images. Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/test-pulp-container-sync.yml \\ ansible/test-pulp-container-publish.yml Updating container image tags in Kayobe configuration (Yoga release and earlier) \u00b6 The image tag used deploy containers may be updated for all images in etc/kayobe/kolla.yml , or for specific images in etc/kayobe/kolla/globals.yml . Currently this is a manual process. Use the new tag from the container image build . For example, to update the default tag for all images (used where no service-specific tag has been set), update etc/kayobe/kolla.yml : # Kolla OpenStack release version. This should be a Docker image tag. # Default is {{ openstack_release }}. kolla_openstack_release : wallaby-20220811T091848 Some or all per-service tags in etc/kayobe/kolla/globals.yml may need to be removed in order to use the new default tag. Alternatively, to update the tag for all containers in a service, update etc/kayobe/kolla/globals.yml : skydive_tag : wallaby-20220811T091848 Alternatively, to update the tag for a specific container, update etc/kayobe/kolla/globals.yml : skydive_analyzer_tag : wallaby-20220811T091848 Updating container image tags in Kayobe configuration (Zed release onwards) \u00b6 The image tags used deploy containers are defined in etc/kayobe/kolla-image-tags.yml . Currently updating these is a manual process. Use the new tag from the container image build . For example, to update the default tag for all images (used where no service-specific tag has been set), update the openstack key, and remove all other keys: # Dict of Kolla image tags to deploy for each service. # Each key is the tag variable prefix name, and the value is another dict, # where the key is the OS distro and the value is the tag to deploy. kolla_image_tags : openstack : rocky-9 : 2024.1-rocky-9-20240101T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240101T000000 Alternatively, update the tag for all containers in a service, e.g. for all nova containers: # Dict of Kolla image tags to deploy for each service. # Each key is the tag variable prefix name, and the value is another dict, # where the key is the OS distro and the value is the tag to deploy. kolla_image_tags : openstack : rocky-9 : 2024.1-rocky-9-20240101T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240101T000000 nova : rocky-9 : 2024.1-rocky-9-20240102T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240102T000000 Alternatively, update the tag for a specific container, e.g. for the nova_compute container: # Dict of Kolla image tags to deploy for each service. # Each key is the tag variable prefix name, and the value is another dict, # where the key is the OS distro and the value is the tag to deploy. kolla_image_tags : openstack : rocky-9 : 2024.1-rocky-9-20240101T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240101T000000 nova_compute : rocky-9 : 2024.1-rocky-9-20240103T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240103T000000 Promoting container images (Zed release onwards) \u00b6 Note This should only be performed when container images are ready for release. The Promote container repositories workflow is triggered automatically when a change is merged to stackhpc-kayobe-config. It may also be run on demand. It runs the following playbooks: dev-pulp-container-tag-query-kayobe.yml : Query the Pulp container image tags defined in a Kayobe configuration repository and set the tag map variable dev_pulp_repository_container_promotion_tags based upon those tags. A path to a Kayobe configuration repository must be specified via kayobe_config_repo_path . dev-pulp-container-promote.yml : Promote a set of container images from stackhpc-dev to stackhpc namespace. The tags to be promoted are defined via dev_pulp_repository_container_promotion_tags . Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-container-tag-query-kayobe.yml \\ ansible/dev-pulp-container-promote.yml \\ -e kayobe_config_repo_path=../stackhpc-kayobe-config/ In this example, the Pulp container image tags defined in the etc/kayobe/kolla-image-tags.yml file in ../stackhpc-kayobe-config repository (relative to the current working directory) will be promoted to releases. Promoting container images (Yoga release and earlier) \u00b6 Note This should only be performed when container images are ready for release. The Promote container repositories (old) workflow runs on demand. It should be run when container images need to be released, typically after a change to update container image tags has been approved. It runs the following playbook: dev-pulp-container-promote-old.yml : Promote a set of container images from stackhpc-dev to stackhpc namespace. The tag to be promoted is defined via dev_pulp_repository_container_promotion_tag which should be specified as an extra variable ( -e ). Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-container-promote-old.yml Other utilities \u00b6 dev-pulp-distribution-list.yml : List available distributions in ark . test-pulp-distribution-list.yml : List available distributions in test . test-repo-test.yml : Install package repositories on the local machine and install some packages.","title":"Content management workflows"},{"location":"usage/content-workflows/#usage-content-management-workflows","text":"Content is primarily managed using the Ansible playbooks and configuration in the stackhpc-release-train repository. The readme provides information on how to install dependencies and run playbooks. The playbooks are designed to be run manually or via a GitHub Actions CI job. This page covers the different workflows available for content management. It may be necessary to combine multiple of these to achieve a desired outcome.","title":"Usage - content management workflows"},{"location":"usage/content-workflows/#add-new-package-repositories-to-release-train","text":"Configuration for package repositories is in: ansible/inventory/group_vars/all/package-repos New package repositories should be added to rpm_package_repos or deb_package_repos in ansible/inventory/group_vars/all/package-repos . The format of these variables is defined in the same file. The following files contain information derived from the above variables and should not need to be modified: ansible/inventory/group_vars/all/dev-pulp-repos ansible/inventory/group_vars/all/test-pulp-repos","title":"Add new package repositories to Release Train"},{"location":"usage/content-workflows/#syncing-package-repositories","text":"The Sync package repositories workflow runs nightly and on demand. It syncs package repositories in Ark with their upstream sources, then creates publications and distributions for any new content: dev-pulp-repo-sync.yml : Synchronise ark with upstream package repositories. dev-pulp-repo-publication-cleanup.yml : Work around an issue with Pulp syncing where multiple publications may exist for a single repository version, breaking the Ansible Squeezer rpm_publication module. dev-pulp-repo-publish.yml : Create development distributions on ark for any new package repository snapshots. Next, it syncs new content in Ark to the test Pulp service: test-pulp-repo-version-query.yml : Query ark for the latest distribution versions and set the version map variable test_pulp_repository_rpm_repo_versions . test-pulp-repo-sync.yml : Synchronise test with ark 's package repositories using the version map variable test_pulp_repository_rpm_repo_versions . test-pulp-repo-publish.yml : Create distributions on test for any new package repository snapshots. It may be necessary to run this workflow on demand if the nightly workflow fails, or if an upstream package has been updated since the last run, or if the repository configuration has been updated. Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-repo-sync.yml \\ ansible/dev-pulp-repo-publication-cleanup.yml \\ ansible/dev-pulp-repo-publish.yml ansible-playbook -i ansible/inventory \\ ansible/test-pulp-repo-version-query.yml \\ ansible/test-pulp-repo-sync.yml \\ ansible/test-pulp-repo-publish.yml If a set of versions other than the latest need to be synced from Ark to test, then it is possible to specify test_pulp_repository_rpm_repo_versions via an extra variables file. In this case, it is not necessary to run test-pulp-repo-version-query.yml . For example: ansible-playbook -i ansible/inventory \\ ansible/test-pulp-repo-sync.yml \\ ansible/test-pulp-repo-publish.yml -e @test_pulp_repository_rpm_repo_versions.yml Here, test_pulp_repository_rpm_repo_versions.yml contains the repository version map variable test_pulp_repository_rpm_repo_versions . It maps package repository short names (in ansible/inventory/group_vars/all/package-repos ) to the version of that repository to sync and publish. For example: test_pulp_repository_rpm_repo_versions: centos_stream_8_appstream: 20211122T102435 centos_stream_8_baseos: 20220101T143229 ...","title":"Syncing package repositories"},{"location":"usage/content-workflows/#promoting-package-repositories","text":"Note This should only be performed when package repositories are ready for release. The Promote package repositories workflow is triggered automatically when a change is merged to stackhpc-kayobe-config. It may also be run on demand. It runs the following playbooks: dev-pulp-repo-version-query-kayobe.yml : Query the Pulp repository versions defined in a Kayobe configuration repository and sets the version map variable dev_pulp_distribution_rpm_promote_versions based upon those versions. A path to a Kayobe configuration repository must be specified via kayobe_config_repo_path . dev-pulp-repo-promote.yml : Promote the set of ark distributions defined in the version map variable dev_pulp_distribution_rpm_promote_versions to releases. Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-repo-version-query-kayobe.yml \\ ansible/dev-pulp-repo-promote.yml \\ -e kayobe_config_repo_path=../stackhpc-kayobe-config/ In this example, the Pulp repository versions defined in the etc/kayobe/pulp-repo-versions.yml file in ../stackhpc-kayobe-config repository (relative to the current working directory) will be promoted to releases. Alternatively, the set of versions to promote may be specified as an extra variables file: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-repo-promote.yml \\ -e @dev_pulp_distribution_rpm_promote_versions.yml Here, dev_pulp_distribution_rpm_promote_versions.yml contains the repository version map variable dev_pulp_distribution_rpm_promote_versions . It maps package repository short names (in ansible/inventory/group_vars/all/package-repos ) to the version of that repository to promote. For example: dev_pulp_distribution_rpm_promote_versions: centos_stream_8_appstream: 20211122T102435 centos_stream_8_baseos: 20220101T143229 ...","title":"Promoting package repositories"},{"location":"usage/content-workflows/#updating-package-repository-versions-in-kayobe-configuration","text":"Note This procedure is expected to change. The Update Kayobe package repository versions workflow runs on demand. It should be run when package repository versions in Kayobe need to be updated. It runs the following playbooks: test-pulp-repo-version-query.yml : Query ark for the latest distribution versions and set the version map variable test_pulp_repository_rpm_repo_versions . test-kayobe-repo-version-generate.yml : Query stackhpc-kayobe-config for the current repository versions, then update them to the previously queried versions in ark . It then stores the new versions YAML file as an artifact, which may be downloaded and manually applied to stackhpc-kayobe-config. In future the workflow will be extended to create a PR. Use GitHub Actions to run this workflow, or to run it manually: ansible/test-pulp-repo-version-query.yml \\ ansible/test-kayobe-repo-version-generate.yml \\ -e kayobe_config_repo_path=./stackhpc-kayobe-config/ Package repository versions are stored in StackHPC Kayobe configuration in etc/kayobe/pulp-repo-versions.yml . Note that the updated versions are not necessarily released. The generated file may be amended as necessary (in case not all updates are required), then copied to the StackHPC Kayobe configuration.","title":"Updating package repository versions in Kayobe configuration"},{"location":"usage/content-workflows/#adding-package-repositories-in-kayobe-configuration","text":"Adding a package repository to the StackHPC Release Train configuration is not sufficient to allow StackHPC OpenStack deployments to use it. The repository must also be defined in StackHPC Kayobe Configuration. We need to define how to sync the package repository from Ark into the local Pulp, as well as how control plane hosts access the repository in the local Pulp service. In the following steps, the short_name of a repository is the short_name field of the repository in ansible/inventory/group_vars/all/package-repos . Add details of the repository to stackhpc_pulp_rpm_repos or stackhpc_pulp_deb_repos in etc/kayobe/pulp.yml to enable syncing of the repository to the local Pulp service. Use the required field to avoid growing sync durations by controlling when the repository needs to be synced. Add a version variable to etc/kayobe/pulp.yml . It should have a format of stackhpc_pulp_repo_<short_name>_version . The version value may be specified or omitted. If omitted, it can be populated automatically using the Update Kayobe package repository versions workflow. Add local Pulp URL and version variables for the repository to etc/kayobe/stackhpc.yml . They should have a format of stackhpc_repo_<short_name>_url and stackhpc_repo_<short_name>_version . Override the repository version variable in stackhpc-ci.yml in the ci-aio , ci-builder and ci-multinode Kayobe environments to use the datestamped version. If the repository needs to be accessible to the host OS of control plane hosts, add it to etc/kayobe/dnf.yml or etc/kayobe/apt.yml . If the repository needs to be accessible to Kolla container images, add it to the Kolla image build configuration section in etc/kayobe/kolla.yml .","title":"Adding package repositories in Kayobe configuration"},{"location":"usage/content-workflows/#building-container-images","text":"Note This procedure is expected to change. The Build Kolla container images workflow in the stackhpc-kayobe-config repository runs on demand. It should be run when new Kolla container images are required. All images may be built, or a specific set of images. All successfully built images will be pushed to Ark, in the stackhpc-dev namespace. An Overcloud container images artifact will be visible on the summary page of a completed successful workflow run. This artifact contains a list of the built images. After a successful container image build workflow, another workflow is triggered to sync the images to the test Pulp. In the following example, the user specified a regular expression of ^magnum , matching all of the Magnum images, and the base image that they depend on. REPOSITORY TAG IMAGE ID CREATED SIZE ark.stackhpc.com/stackhpc-dev/magnum-api 2024.1-rocky-9-20240811T091848 32f2b9299194 6 minutes ago 1.29GB ark.stackhpc.com/stackhpc-dev/magnum-conductor 2024.1-rocky-9-20240811T091848 35e4c1cda1a8 7 minutes ago 1.14GB ark.stackhpc.com/stackhpc-dev/magnum-base 2024.1-rocky-9-20240811T091848 3bd5f3e50aa3 7 minutes ago 1.14GB ark.stackhpc.com/stackhpc-dev/openstack-base 2024.1-rocky-9-20240811T091848 bd02fa0ec1d6 7 minutes ago 991MB ark.stackhpc.com/stackhpc-dev/base 2024.1-rocky-9-20240811T091848 bd02fa0ec1d6 7 minutes ago 991MB In this example, the base , openstack-base and Magnum images have been tagged 2024.1-rocky-9-20240811T091848 . Instructions for building Kolla container images manually are provided in the StackHPC kayobe config documentation .","title":"Building container images"},{"location":"usage/content-workflows/#publishing-container-images","text":"The Publish container repositories workflow runs after a push to the main branch of the stackhpc-release-train repository, when relevant files have changed. It runs the following playbooks: dev-pulp-container-publish.yml : Configure access control for development and release container repositories and distributions on ark . Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-container-publish.yml Configuration for container images is in: ansible/inventory/group_vars/all/kolla . ansible/inventory/group_vars/all/dev-pulp-containers . ansible/inventory/group_vars/all/test-pulp-containers . New images should be added to kolla_container_images in ansible/inventory/group_vars/all/kolla .","title":"Publishing container images"},{"location":"usage/content-workflows/#syncing-container-images","text":"The Sync container repositories workflow runs after a successful container image build workflow run, or on demand. It runs the following playbooks: test-pulp-container-sync.yml : Synchronise test with container images from stackhpc-dev namespace on ark . test-pulp-container-publish.yml : Create distributions on test Pulp server for any new container images. Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/test-pulp-container-sync.yml \\ ansible/test-pulp-container-publish.yml","title":"Syncing container images"},{"location":"usage/content-workflows/#updating-container-image-tags-in-kayobe-configuration-yoga-release-and-earlier","text":"The image tag used deploy containers may be updated for all images in etc/kayobe/kolla.yml , or for specific images in etc/kayobe/kolla/globals.yml . Currently this is a manual process. Use the new tag from the container image build . For example, to update the default tag for all images (used where no service-specific tag has been set), update etc/kayobe/kolla.yml : # Kolla OpenStack release version. This should be a Docker image tag. # Default is {{ openstack_release }}. kolla_openstack_release : wallaby-20220811T091848 Some or all per-service tags in etc/kayobe/kolla/globals.yml may need to be removed in order to use the new default tag. Alternatively, to update the tag for all containers in a service, update etc/kayobe/kolla/globals.yml : skydive_tag : wallaby-20220811T091848 Alternatively, to update the tag for a specific container, update etc/kayobe/kolla/globals.yml : skydive_analyzer_tag : wallaby-20220811T091848","title":"Updating container image tags in Kayobe configuration (Yoga release and earlier)"},{"location":"usage/content-workflows/#updating-container-image-tags-in-kayobe-configuration-zed-release-onwards","text":"The image tags used deploy containers are defined in etc/kayobe/kolla-image-tags.yml . Currently updating these is a manual process. Use the new tag from the container image build . For example, to update the default tag for all images (used where no service-specific tag has been set), update the openstack key, and remove all other keys: # Dict of Kolla image tags to deploy for each service. # Each key is the tag variable prefix name, and the value is another dict, # where the key is the OS distro and the value is the tag to deploy. kolla_image_tags : openstack : rocky-9 : 2024.1-rocky-9-20240101T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240101T000000 Alternatively, update the tag for all containers in a service, e.g. for all nova containers: # Dict of Kolla image tags to deploy for each service. # Each key is the tag variable prefix name, and the value is another dict, # where the key is the OS distro and the value is the tag to deploy. kolla_image_tags : openstack : rocky-9 : 2024.1-rocky-9-20240101T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240101T000000 nova : rocky-9 : 2024.1-rocky-9-20240102T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240102T000000 Alternatively, update the tag for a specific container, e.g. for the nova_compute container: # Dict of Kolla image tags to deploy for each service. # Each key is the tag variable prefix name, and the value is another dict, # where the key is the OS distro and the value is the tag to deploy. kolla_image_tags : openstack : rocky-9 : 2024.1-rocky-9-20240101T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240101T000000 nova_compute : rocky-9 : 2024.1-rocky-9-20240103T000000 ubuntu-jammy : 2024.1-ubuntu-jammy-20240103T000000","title":"Updating container image tags in Kayobe configuration (Zed release onwards)"},{"location":"usage/content-workflows/#promoting-container-images-zed-release-onwards","text":"Note This should only be performed when container images are ready for release. The Promote container repositories workflow is triggered automatically when a change is merged to stackhpc-kayobe-config. It may also be run on demand. It runs the following playbooks: dev-pulp-container-tag-query-kayobe.yml : Query the Pulp container image tags defined in a Kayobe configuration repository and set the tag map variable dev_pulp_repository_container_promotion_tags based upon those tags. A path to a Kayobe configuration repository must be specified via kayobe_config_repo_path . dev-pulp-container-promote.yml : Promote a set of container images from stackhpc-dev to stackhpc namespace. The tags to be promoted are defined via dev_pulp_repository_container_promotion_tags . Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-container-tag-query-kayobe.yml \\ ansible/dev-pulp-container-promote.yml \\ -e kayobe_config_repo_path=../stackhpc-kayobe-config/ In this example, the Pulp container image tags defined in the etc/kayobe/kolla-image-tags.yml file in ../stackhpc-kayobe-config repository (relative to the current working directory) will be promoted to releases.","title":"Promoting container images (Zed release onwards)"},{"location":"usage/content-workflows/#promoting-container-images-yoga-release-and-earlier","text":"Note This should only be performed when container images are ready for release. The Promote container repositories (old) workflow runs on demand. It should be run when container images need to be released, typically after a change to update container image tags has been approved. It runs the following playbook: dev-pulp-container-promote-old.yml : Promote a set of container images from stackhpc-dev to stackhpc namespace. The tag to be promoted is defined via dev_pulp_repository_container_promotion_tag which should be specified as an extra variable ( -e ). Use GitHub Actions to run this workflow, or to run it manually: ansible-playbook -i ansible/inventory \\ ansible/dev-pulp-container-promote-old.yml","title":"Promoting container images (Yoga release and earlier)"},{"location":"usage/content-workflows/#other-utilities","text":"dev-pulp-distribution-list.yml : List available distributions in ark . test-pulp-distribution-list.yml : List available distributions in test . test-repo-test.yml : Install package repositories on the local machine and install some packages.","title":"Other utilities"},{"location":"usage/github-organisation-management/","text":"Usage - GitHub Organisation Management \u00b6 With the the ever growing number of repositories related to StackHPC Release Train, it is important that we can manage all of the repositories in an automated manner. To achieve this we have opted to use Terraform and its GitHub Provider to automate the configuration of StackHPC's GitHub profile and repositories . By using Terraform's GitHub Provider we can configure various aspects of GitHub ranging from enforcing branch protection rules across repositories, creating teams and assigning members in addition to configuring how pull requests (PRs) are reviewed and merged. The Terraform configuration can be found within the stackhpc-release-train repository under terraform/github . It is expected that all plans and applies are carried out within GitHub Actions where the statefile is stored within Terraform Cloud . Note Access to Terraform Cloud is limited due to team size restrictions if you need access to Terraform Cloud and don't have access already then feel free to request access in the appropriate Slack channel. GitHub authentication is handled using a GitHub app. Making Changes \u00b6 In this section we shall look at how you may modify the Terraform configuration to suit your needs and requirements. This will not cover how to increase the use of the GitHub Provider to configure others elements of the organisation. However, if you need to use additional parts of the provider please consult the provider's documention . Import, Plan & Apply \u00b6 Note Terraform by default will attempt to delete resources that are contained within its statefile yet no longer represented in the plan. This is not ideal for the use case of managing repositories. Therefore, lifecycle prevent destroy has been applied to all resources within this configuration. This will cause any plan that attempts to delete a resource to fail. Therefore, if you have a reason to remove and delete something such as team member then this must be done as described in Removing Resources The workflow for altering the configuration is as follows; Make the change to terraform.tfvars.json and potentially one of the .tf files Open a PR which shall trigger a GitHub workflow which shall produce a plan detailing the changes Terraform shall make Review the changes If any of the changes involve adopting an existing repository or team then you must run terraform_github_import.yml , see below for details If you do import existing resources then you must produce a new plan which currently can achieved by closing the PR and reopening it Merge once the PR has been approved this will trigger terraform-github.yml to perform the apply Importing Resources \u00b6 Whilst Terraform is capable of creating resources it is expected that a lot of the resources already exist and therefore need to be imported before Terraform can apply the configuration rules. To import a resource we have made available a convenient GitHub Workflow called Terrform GitHub Import which will identify what resources are referenced within the configuration and are missing from the statefile . To use the workflow you will navigate to Terraform GitHub Import under the Actions tab for stackhpc-release-train. Once there you can manually dispatch a workflow where you must specify the branch for which the workflow will run from this should be the branch that has an open PR for. Also you can specify if the import should perform a dry run to provide reassurance before making actually changes to the statefile. You must run the workflow again with Dry Run unticked. Warning As we currently use one workspace to manage the statefile it can lead to difficulties when attempting to make multiple changes across different PRs. Therefore, it is currently recommended that merges are made shortly after an import has taken place. Otherwise it will negatively impact the plans produced due to conflicts between statefile and the various configurations. Also note that if an import has been carried in error then you must make sure to remove that from the statefile which can achieved using the instructions found in Removing Resources Adding Member to Team \u00b6 To add a member to a team you must add the new member's username to the the users object within the specific team within teams object. Majority of users can be placed within the members object however certain users such as those that are administrators of the organisation must be placed within the maintainers object. Also either the maintainers or members must contain at least one entry this is due to a limitation of the provider. A fix would be to include stackhpc-ci if there is currently no one in the team. terraform/github/terraform.tfvars.json \"Batch\": { \"description\": \"Team responsible for Batch development\", \"privacy\": \"closed\", \"users\": { \"maintainers\": [], \"members\": [ \"jackhodgkiss\" # New user added to team ] } } Adding Repository to Team \u00b6 To add a repository to a team you can append the name of the repository to the desired team within the repositories object. terraform/github/terraform.tfvars.json \"repositories\": { \"Ansible\": [ \"ansible-role-os-networks\", ... \"ansible-collection-pulp\", \"ansible-collection-hashicorp\", \"ansible-collection-openstack-ops\", \"ansible-role-vxlan\" # New repository added to team ] Removing Resources \u00b6 To remove a resource you must first identify and delete the entry from within terraform.tfvars.json . Once done you can open a PR which will trigger the terrform-github workflow which will most likely return an error due to the attempt to delete the resource via Terraform being block by the lifecycle prevent_destroy attribute. To resolve this issue you will need access to the Terraform Cloud GitHub workspace, if you don't have access then you may request access or have someone who does have access to perform the removal on your behalf. Prior to remove a resource you must ensure that you have the Terraform CLI tools and have authenticated with Terraform Cloud, instructions for this can be found here . The steps for removing a resource such as the ansible-role-vxlan for the Ansible team are as follows; git clone git@github.com:stackhpc/stackhpc-release-train.git cd stackhpc-release-train/terraform/github terraform state rm 'github_branch_protection.ansible_branch_protection[\"ansible-role-vxlan\"]' The general form for state rm is ${resource_address}[\"resource_id\"] . You can found out more about the state rm command here here . You may also find documentation about the GitHub provider which may provide insight into how the resources are composed here Renaming Resources \u00b6 To rename a resource you must first identify and rename the entry within terraform.tfvars.json . Without further intervention Terraform will see this as deletion of one resource and creation of another, and would typically return a failure when trying to create the resource that already exists. To resolve this issue you will need access to the Terraform Cloud GitHub workspace, if you don't have access then you may request access or have someone who does have access to perform the rename on your behalf. Prior to renaming a resource you must ensure that you have the Terraform CLI tools and have authenticated with Terraform Cloud, instructions for this can be found here . In the following example we will rename the ansible-role-os-networks repository to os-networks . Clone this repository and change to the terraform/github directory: git clone git@github.com:stackhpc/stackhpc-release-train.git cd stackhpc-release-train/terraform/github Rename the ansible-role-os-networks entry in terraform.tfvars.json . Commit the change and push to a branch on GitHub. Create a script named rename-repo.sh with the following content: #!/bin/bash -eux repo_src = ${ 1 :?Repo source } repo_dst = ${ 2 :?Repo destination } # e.g. -dry-run args = \"\" # This resource list will vary depending on the repository. # In particurlar this may include the branch protection and team associations. resources = \"github_branch_protection.ansible_branch_protection \\ github_issue_label.community_files_label \\ github_issue_label.stackhpc_ci_label \\ github_issue_label.workflows_label \\ github_team_repository.admin_repositories \\ github_team_repository.ansible_repositories \\ github_team_repository.developers_repositories \\ github_repository.repositories\" for resource in $resources ; do terraform state mv $args \" $resource [\\\" $repo_src \\\"]\" \" $resource [\\\" $repo_dst \\\"]\" done Edit the resources list based on the output of terraform state list | grep <repo> Make the script executable: chmod +x rename-repo.sh Run the script to rename the repository. Note that this will directly update the state file in Terraform cloud: ./rename-repo.sh ansible-role-os-networks os-networks Create a PR for the changes. StackHPC Release Train TF bot \u00b6 GitHub authentication is handled using the StackHPC Release Train TF bot App . This app has a private key that is registered as a GitHub secret . The app is installed on the stackhpc organisation, with access to all repositories. It has only the necessary permissions, but these are rather broad. GitHub apps are documented here .","title":"GitHub Organisation Management"},{"location":"usage/github-organisation-management/#usage-github-organisation-management","text":"With the the ever growing number of repositories related to StackHPC Release Train, it is important that we can manage all of the repositories in an automated manner. To achieve this we have opted to use Terraform and its GitHub Provider to automate the configuration of StackHPC's GitHub profile and repositories . By using Terraform's GitHub Provider we can configure various aspects of GitHub ranging from enforcing branch protection rules across repositories, creating teams and assigning members in addition to configuring how pull requests (PRs) are reviewed and merged. The Terraform configuration can be found within the stackhpc-release-train repository under terraform/github . It is expected that all plans and applies are carried out within GitHub Actions where the statefile is stored within Terraform Cloud . Note Access to Terraform Cloud is limited due to team size restrictions if you need access to Terraform Cloud and don't have access already then feel free to request access in the appropriate Slack channel. GitHub authentication is handled using a GitHub app.","title":"Usage - GitHub Organisation Management"},{"location":"usage/github-organisation-management/#making-changes","text":"In this section we shall look at how you may modify the Terraform configuration to suit your needs and requirements. This will not cover how to increase the use of the GitHub Provider to configure others elements of the organisation. However, if you need to use additional parts of the provider please consult the provider's documention .","title":"Making Changes"},{"location":"usage/github-organisation-management/#import-plan-apply","text":"Note Terraform by default will attempt to delete resources that are contained within its statefile yet no longer represented in the plan. This is not ideal for the use case of managing repositories. Therefore, lifecycle prevent destroy has been applied to all resources within this configuration. This will cause any plan that attempts to delete a resource to fail. Therefore, if you have a reason to remove and delete something such as team member then this must be done as described in Removing Resources The workflow for altering the configuration is as follows; Make the change to terraform.tfvars.json and potentially one of the .tf files Open a PR which shall trigger a GitHub workflow which shall produce a plan detailing the changes Terraform shall make Review the changes If any of the changes involve adopting an existing repository or team then you must run terraform_github_import.yml , see below for details If you do import existing resources then you must produce a new plan which currently can achieved by closing the PR and reopening it Merge once the PR has been approved this will trigger terraform-github.yml to perform the apply","title":"Import, Plan &amp; Apply"},{"location":"usage/github-organisation-management/#importing-resources","text":"Whilst Terraform is capable of creating resources it is expected that a lot of the resources already exist and therefore need to be imported before Terraform can apply the configuration rules. To import a resource we have made available a convenient GitHub Workflow called Terrform GitHub Import which will identify what resources are referenced within the configuration and are missing from the statefile . To use the workflow you will navigate to Terraform GitHub Import under the Actions tab for stackhpc-release-train. Once there you can manually dispatch a workflow where you must specify the branch for which the workflow will run from this should be the branch that has an open PR for. Also you can specify if the import should perform a dry run to provide reassurance before making actually changes to the statefile. You must run the workflow again with Dry Run unticked. Warning As we currently use one workspace to manage the statefile it can lead to difficulties when attempting to make multiple changes across different PRs. Therefore, it is currently recommended that merges are made shortly after an import has taken place. Otherwise it will negatively impact the plans produced due to conflicts between statefile and the various configurations. Also note that if an import has been carried in error then you must make sure to remove that from the statefile which can achieved using the instructions found in Removing Resources","title":"Importing Resources"},{"location":"usage/github-organisation-management/#adding-member-to-team","text":"To add a member to a team you must add the new member's username to the the users object within the specific team within teams object. Majority of users can be placed within the members object however certain users such as those that are administrators of the organisation must be placed within the maintainers object. Also either the maintainers or members must contain at least one entry this is due to a limitation of the provider. A fix would be to include stackhpc-ci if there is currently no one in the team. terraform/github/terraform.tfvars.json \"Batch\": { \"description\": \"Team responsible for Batch development\", \"privacy\": \"closed\", \"users\": { \"maintainers\": [], \"members\": [ \"jackhodgkiss\" # New user added to team ] } }","title":"Adding Member to Team"},{"location":"usage/github-organisation-management/#adding-repository-to-team","text":"To add a repository to a team you can append the name of the repository to the desired team within the repositories object. terraform/github/terraform.tfvars.json \"repositories\": { \"Ansible\": [ \"ansible-role-os-networks\", ... \"ansible-collection-pulp\", \"ansible-collection-hashicorp\", \"ansible-collection-openstack-ops\", \"ansible-role-vxlan\" # New repository added to team ]","title":"Adding Repository to Team"},{"location":"usage/github-organisation-management/#removing-resources","text":"To remove a resource you must first identify and delete the entry from within terraform.tfvars.json . Once done you can open a PR which will trigger the terrform-github workflow which will most likely return an error due to the attempt to delete the resource via Terraform being block by the lifecycle prevent_destroy attribute. To resolve this issue you will need access to the Terraform Cloud GitHub workspace, if you don't have access then you may request access or have someone who does have access to perform the removal on your behalf. Prior to remove a resource you must ensure that you have the Terraform CLI tools and have authenticated with Terraform Cloud, instructions for this can be found here . The steps for removing a resource such as the ansible-role-vxlan for the Ansible team are as follows; git clone git@github.com:stackhpc/stackhpc-release-train.git cd stackhpc-release-train/terraform/github terraform state rm 'github_branch_protection.ansible_branch_protection[\"ansible-role-vxlan\"]' The general form for state rm is ${resource_address}[\"resource_id\"] . You can found out more about the state rm command here here . You may also find documentation about the GitHub provider which may provide insight into how the resources are composed here","title":"Removing Resources"},{"location":"usage/github-organisation-management/#renaming-resources","text":"To rename a resource you must first identify and rename the entry within terraform.tfvars.json . Without further intervention Terraform will see this as deletion of one resource and creation of another, and would typically return a failure when trying to create the resource that already exists. To resolve this issue you will need access to the Terraform Cloud GitHub workspace, if you don't have access then you may request access or have someone who does have access to perform the rename on your behalf. Prior to renaming a resource you must ensure that you have the Terraform CLI tools and have authenticated with Terraform Cloud, instructions for this can be found here . In the following example we will rename the ansible-role-os-networks repository to os-networks . Clone this repository and change to the terraform/github directory: git clone git@github.com:stackhpc/stackhpc-release-train.git cd stackhpc-release-train/terraform/github Rename the ansible-role-os-networks entry in terraform.tfvars.json . Commit the change and push to a branch on GitHub. Create a script named rename-repo.sh with the following content: #!/bin/bash -eux repo_src = ${ 1 :?Repo source } repo_dst = ${ 2 :?Repo destination } # e.g. -dry-run args = \"\" # This resource list will vary depending on the repository. # In particurlar this may include the branch protection and team associations. resources = \"github_branch_protection.ansible_branch_protection \\ github_issue_label.community_files_label \\ github_issue_label.stackhpc_ci_label \\ github_issue_label.workflows_label \\ github_team_repository.admin_repositories \\ github_team_repository.ansible_repositories \\ github_team_repository.developers_repositories \\ github_repository.repositories\" for resource in $resources ; do terraform state mv $args \" $resource [\\\" $repo_src \\\"]\" \" $resource [\\\" $repo_dst \\\"]\" done Edit the resources list based on the output of terraform state list | grep <repo> Make the script executable: chmod +x rename-repo.sh Run the script to rename the repository. Note that this will directly update the state file in Terraform cloud: ./rename-repo.sh ansible-role-os-networks os-networks Create a PR for the changes.","title":"Renaming Resources"},{"location":"usage/github-organisation-management/#stackhpc-release-train-tf-bot","text":"GitHub authentication is handled using the StackHPC Release Train TF bot App . This app has a private key that is registered as a GitHub secret . The app is installed on the stackhpc organisation, with access to all repositories. It has only the necessary permissions, but these are rather broad. GitHub apps are documented here .","title":"StackHPC Release Train TF bot"},{"location":"usage/notifications/","text":"Notifications \u00b6 Much of the functionality of StackHPC Release Train is built around GitHub Actions workflows. Some of these are triggered automatically based on an event such as pushing to a branch in a GitHub repository. Others are triggered manually, such as using a \"workflow dispatch\" from GitHub's web UI or API. Failure of a manually triggered workflow will result in an email being sent to the user who triggered the workflow. Failure of an automatically triggered workflow will result in an email being sent to the user who created the workflow file. This is not ideal, and makes that person a bottleneck and single point of failure. To make failures of automatically triggered workflows more visible, notifications are sent to the #release-train-alerts Slack channel. These notifications are implemented in the slack-alert GitHub action. The slack-alert action uses the \"workflow builder\" approach described in slack-github-action . Slack's workflow builder feature allows for flexible integration of Slack with various other services, based on various events. The Release train status workflow has a webhook URL event trigger, with a single action that sends a message to the #release-train-alerts Slack channel. The Slack webhook URL is set in the SLACK_WEBHOOK_URL GitHub Actions secret, and the #release-train-alerts channel ID is set in the SLACK_CHANNEL_ID GitHub Actions variable.","title":"Notifications"},{"location":"usage/notifications/#notifications","text":"Much of the functionality of StackHPC Release Train is built around GitHub Actions workflows. Some of these are triggered automatically based on an event such as pushing to a branch in a GitHub repository. Others are triggered manually, such as using a \"workflow dispatch\" from GitHub's web UI or API. Failure of a manually triggered workflow will result in an email being sent to the user who triggered the workflow. Failure of an automatically triggered workflow will result in an email being sent to the user who created the workflow file. This is not ideal, and makes that person a bottleneck and single point of failure. To make failures of automatically triggered workflows more visible, notifications are sent to the #release-train-alerts Slack channel. These notifications are implemented in the slack-alert GitHub action. The slack-alert action uses the \"workflow builder\" approach described in slack-github-action . Slack's workflow builder feature allows for flexible integration of Slack with various other services, based on various events. The Release train status workflow has a webhook URL event trigger, with a single action that sends a message to the #release-train-alerts Slack channel. The Slack webhook URL is set in the SLACK_WEBHOOK_URL GitHub Actions secret, and the #release-train-alerts channel ID is set in the SLACK_CHANNEL_ID GitHub Actions variable.","title":"Notifications"},{"location":"usage/secrets/","text":"Secrets \u00b6 Various GitHub Actions secrets are used within StackHPC Release Train for integrating with external services. All secrets are scoped to the StackHPC Release Train repository unless stated otherwise. Secret Type Owner Description ANSIBLE_VAULT_PASSWORD Ansible vault password N/A Ansible Vault password for StackHPC Release Train secrets. GALAXY_API_KEY Ansible Galaxy API token stackhpc-ci GitHub user Organisation secret used for importing Ansible content into Ansible Galaxy. repository_configuration_token GitHub PAT token stackhpc-ci GitHub user Used in source code CI to create GitHub pull requests. Used in GitHub organisation management to add comments to PRs. SLACK_WEBHOOK_URL Slack webhook URL Infra team leads Used to send Slack notifications on GitHub Actions workflow failures. TF_API_TOKEN Terraform Cloud API token Jack Used in GitHub organisation management to authenticate with Terraform cloud. TF_VAR_GITHUB_APP_PEM_FILE GitHub app PEM file GitHub org admins Used in GitHub organisation management to authorise Terraform to manage GitHub repositories.","title":"Secrets"},{"location":"usage/secrets/#secrets","text":"Various GitHub Actions secrets are used within StackHPC Release Train for integrating with external services. All secrets are scoped to the StackHPC Release Train repository unless stated otherwise. Secret Type Owner Description ANSIBLE_VAULT_PASSWORD Ansible vault password N/A Ansible Vault password for StackHPC Release Train secrets. GALAXY_API_KEY Ansible Galaxy API token stackhpc-ci GitHub user Organisation secret used for importing Ansible content into Ansible Galaxy. repository_configuration_token GitHub PAT token stackhpc-ci GitHub user Used in source code CI to create GitHub pull requests. Used in GitHub organisation management to add comments to PRs. SLACK_WEBHOOK_URL Slack webhook URL Infra team leads Used to send Slack notifications on GitHub Actions workflow failures. TF_API_TOKEN Terraform Cloud API token Jack Used in GitHub organisation management to authenticate with Terraform cloud. TF_VAR_GITHUB_APP_PEM_FILE GitHub app PEM file GitHub org admins Used in GitHub organisation management to authorise Terraform to manage GitHub repositories.","title":"Secrets"},{"location":"usage/source-code-ci/","text":"Usage - source code continuous integration \u00b6 Source code continuous integration (CI) is handled by Github Workflows. There are currently three workflows in use, whose objective is to perform tedious tasks or to ensure that the code is correct and style guidelines are being followed. A brief overview of these workflows was given in the Overview Section whereas this section will provide additional insight into how these workflows function and how you can modify these workflows. Also discussed are the community files used by Github to improve developer experience within the respositories in addition to how we intend to synchronise all of the source code repositories with latests files. Github Workflows \u00b6 This section is simply intended to document the behaviour of these workflows actual modification should be handled via the Synchronise Repositories Playbook which is documented below in Synchronise Repositories Playbook Section . The table below contains the different workflows with a description of each and the project type they would are used within. Workflow Description Project Type Upstream Sync Keep our downstream fork up-to-date with the upstream counterpart OpenStack Tag & Release Generate an new tag and accompanying release whenever a commit is pushed to select branches OpenStack Tox Perform linting and unit testing OpenStack Lint Collection Perform linting and sanity checks on Ansible collections Ansible (Collection) Publish Role Publish the Ansible Role on Ansible Galaxy Ansible (Role) Publish Collection Publish the Ansible Collection on Ansible Galaxy Ansible (Collection) Reusable Workflow Location The following workflows are setup as reusable workflows and can be found within the StackHPC/.github repository. The advantage of this approach means that changes to a given workflow can be made in one place rather than each repository individualy. Tox \u00b6 OpenStack use Tox to manage the unit tests and style checks for the various projects they maintain. Therefore, when a pull request is opened the tox workflow will automatically perform a series of unit tests and linting in order ensure correctness and style guidelines are being met. The python environment will depend on the branch. Python 3.8, 3.10, and 3.12 are tested for various releases. This can be controlled within the strategy matrix of the workflow. The Python versions should correspond to those used in the supported OS distributions for a particular release. The source for the workflow can be found here . It is managed centrally and imported into all downstream branches. Tag & Release \u00b6 Software that depends on the source code repositories that the Release Train manages, typically will use a tag to identify the particular release to download and use. Therefore, to automate the process this workflow will generate a new tag for the latest commit to stackhpc/** which in turn will publish a new release under the given repository's Releases page. Warning The tag format currently used by this workflow is stackhpc/a.b.c.d which is NOT SemVer compliant and cannot be used in certain circumstances such as within Helm Charts . However, for the needs of the Release Train this is adequate. Upstream Sync \u00b6 Since many of our repositories are forked from OpenStack we need to ensure that we remain in sync with upstream in order to prevent situations where we deviate which could make any future attempts more difficult than it should be. Therefore, this workflow will periodically check if our stackhpc/** are behind their OpenStack counterparts. If so the workflow will copy the OpenStack branch into the repository and then make a pull request off of this copy ready to merge pending review by the relevant codeowner. Since the workflow uses the Github REST API it will still be able to open a PR even if it would result in a merge conflict allowing the relevant codeowner to make the necessary changes to resolve such a conflict. The workflow can be triggered manually within the Actions tab of a repository in addition to being scheduled to automatically. Currently is scheduled to run once a week on Monday at 09:15AM BST . This can be changed in the workflow template within the .github repository . As the workflow scheduled it must be located within the default branch such as main or master for Github to register it. 'on' : schedule : - cron : '15 8 * * 1' workflow_dispatch : Lint Collection \u00b6 Ansible collections are linted using Ansible lint , and sanity checked using the ansible-test sanity command. These checks run against pull requests to Ansible collection repositories and use a matrix to test multiple versions of Ansible. The tested versions of Ansible should generally correspond to those used by supported versions of Kayobe and Kolla. Publish Collection/Role \u00b6 These two workflow automate the publication of a Ansible Collection or Role to Ansible Galaxy. They can triggered either manually or whenever a new tag is pushed in the form v[0-9]+.[0-9]+.[0-9]+ for a collection or v?[0-9]+.[0-9]+.[0-9]+ for a role. Synchronise Repositories Playbook \u00b6 Whilst the workflows can be imported manually this is NOT the intended approach, rather we will use an Ansible Playbook responsible for identifying repositories that are out of sync or missing the required files and perform pull requests to bring them inline. The playbook and role called source-repo-sync is located within this repository and will run inside of a Github Workflow, whenever a change is made to the configuration or variables. The motivation behind this is to have single-source of truth and free up developer time as creating dozens of branches and pull requests can be quite monotonous. Making modifications to the playbook \u00b6 Source Repositories Vars default_releases : list of OpenStack release series currently supported by StackHPC and used within the workflows openstack_workflows : dictionary of OpenStack specific workflows as mentioned above default_branch_only: list of workflows that will only exist on the default branch (master/main) elsewhere: list of workflows that will be placed on other branches such as stackhpc/xena see default_releases ansible_workflows : dictionary that contains either the type of Ansible collection or role collection: list of workflows specific to Ansible collections role: list of workflows specific to Ansible roles community_files : dictionary of templates for various community files. Each community file can have multiple templates are can be written as multiline strings. source_repositories : dictionary of repositories targeted by the Ansible playbook repository_name: points to repository, must match the name of the repository exactly. repository_type: used to distinguish if the repository is OpenStack, Ansible or branchless, defaults to OpenStack if not provided workflows: can either use openstack_workflows , ansible_workflows or provide a custom set of workflows, see ** below! ** --- default_releases : - xena - wallaby - victoria openstack_workflows : default_branch_only : - upstream-sync elsewhere : - tox - tag-and-release ansible_workflows : collection : - publish_collection role : - publish_role community_files : codeowners : kayobe_codeowners : | * @stackhpc/kayobe ansible_codeowners : | * @stackhpc/ansible source_repositories : kolla : community_files : - codeowners : content : '{{ community_files.codeowners.kayobe_codeowners }}' dest : '.github/CODEOWNERS' barbican : ignored_releases : - victoria - xena stackhpc-inspector-plugins : repository_type : 'branchless' workflows : '{{ openstack_workflows.elsewhere }}' ansible-role-os-networks : repository_type : 'ansible' workflows : '{{ ansible_workflows.role }}' ansible-collection-cephadm : repository_type : 'ansible' workflows : '{{ ansible_workflows.collection }}' In this subsection we shall explore how to make various modifications to the playbook. Please review the Source Repositories Vars for a description of the variables found within source-repositories vars file. Note Modifications can be made either by clone the stackhpc-release-train repository locally or using the Github Dev Editor by pressing . key within the repository. Add new repository \u00b6 To add new repositories to be handled by this playbook you can edit source-repositories . Identify the source_repositories dictionary and insert your new repository. For example the below code snippet will add neutron to the source repo sync all default workflows and community files. Also all release series will be ignored except yoga . source_repositories : neutron : ignored_releases : - xena - wallaby - victoria community_files : - codeowners : content : \"{{ community_files.codeowners.openstack }}\" dest : \".github/CODEOWNERS\" Note Please refer to Making modifications to the playbook for description of changes that can be made. Changing the release series \u00b6 To change the release series for all OpenStack repositories this can be achived by editing the default_releases variable. For example if you wanted to remove victoria and add support for yoga. Once this change has been merged into the main branch it shall perform a series of pull requests updating the workflows across all listed repositories. ansible/inventory/group_vars/all/source-repositories --- default_releases : - xena - wallaby - victoria --- default_releases : - yoga - xena - wallaby Adding new workflows \u00b6 It is more involved to add additional workflows for distribution across the repositories. Add the workflow to the .github repository as one that is reusable. Add the workflow caller as Jinja template in the folder ansible/roles/source-repo-sync/templates such as deploy.jinja Update ansible/inventory/group_vars/all/source-repositories either by changing the default OpenStack or Ansible dict, which would propagate to all repositories of that type or updating only the repositories workflows dict directly. Use the name of the jinja template. See below. ansible/inventory/group_vars/all/source-repositories If you wish to update all repositories: openstack_workflows : default_branch_only : - deploy - upstream-sync elsewhere : - tox - tag-and-release If you want the new additional workflow to only be included in specific repository: source_repositories : kolla : workflows : additional_workflows : - deploy Remove or omit workflows \u00b6 To remove a workflow from being deployed or updated across all repositories simply remove it from either openstack_workflows or ansible_workflows dict. If you would like to remove a workflow from a specific repository either because it is not necessary or you would prefer to manage that workflow within the repository itself then you can use ignore_workflows dict within the target repository. Note however, this will not create a pull request to remove it from the repositories if it has already been deployed. ansible/inventory/group_vars/all/source-repositories If you wish to remove a workflow from being deployed or updated for all repositories: openstack_workflows : default_branch_only : - upstream-sync elsewhere : - tag-and-release If you wish to remove a workflow from being deployed or updated to a specific repository: source_repositories : kolla : workflows : ignored_workflows : elsewhere : - tox Managing Community Files \u00b6 Github provides support for many community files that can be used throughout the release train repositories. Currently we support CODEOWNERS files which will automatically assign a team or team member to a pull request that relates to code that they own. All community files are stored in the community_files dictionary whereby the key is the name of file and then the value is a list of templates stored as multiline strings. To add a new community file or template simply expand the dictionary where appropriate and ensure that the relevant repositories are updated within the source_repositories dictionary. source_repositories : kolla : community_files : - codeowners : content : '{{ community_files.codeowners.kayobe_codeowners }}' dest : '.github/CODEOWNERS' Additional Notes \u00b6 All commits and pull requests are authored by the stackhpc-ci bot. The bots personal access token has been added to the secrets of this repository. If a branch or pull request originating from the source repositories playbook is not closed or deleted, prior to running further additonal changes then the bot will automatically delete the branch and pull request making room for the new changes. The source repositories playbook will NOT delete unmanaged files or branches. For example when we drop support for stackhpc/victoria the playbook will NOT delete the workflows from that branch. Due to the extensive use of the Github API the playbook will pause after every pull request for 10 seconds.","title":"Source code CI"},{"location":"usage/source-code-ci/#usage-source-code-continuous-integration","text":"Source code continuous integration (CI) is handled by Github Workflows. There are currently three workflows in use, whose objective is to perform tedious tasks or to ensure that the code is correct and style guidelines are being followed. A brief overview of these workflows was given in the Overview Section whereas this section will provide additional insight into how these workflows function and how you can modify these workflows. Also discussed are the community files used by Github to improve developer experience within the respositories in addition to how we intend to synchronise all of the source code repositories with latests files.","title":"Usage - source code continuous integration"},{"location":"usage/source-code-ci/#github-workflows","text":"This section is simply intended to document the behaviour of these workflows actual modification should be handled via the Synchronise Repositories Playbook which is documented below in Synchronise Repositories Playbook Section . The table below contains the different workflows with a description of each and the project type they would are used within. Workflow Description Project Type Upstream Sync Keep our downstream fork up-to-date with the upstream counterpart OpenStack Tag & Release Generate an new tag and accompanying release whenever a commit is pushed to select branches OpenStack Tox Perform linting and unit testing OpenStack Lint Collection Perform linting and sanity checks on Ansible collections Ansible (Collection) Publish Role Publish the Ansible Role on Ansible Galaxy Ansible (Role) Publish Collection Publish the Ansible Collection on Ansible Galaxy Ansible (Collection) Reusable Workflow Location The following workflows are setup as reusable workflows and can be found within the StackHPC/.github repository. The advantage of this approach means that changes to a given workflow can be made in one place rather than each repository individualy.","title":"Github Workflows"},{"location":"usage/source-code-ci/#tox","text":"OpenStack use Tox to manage the unit tests and style checks for the various projects they maintain. Therefore, when a pull request is opened the tox workflow will automatically perform a series of unit tests and linting in order ensure correctness and style guidelines are being met. The python environment will depend on the branch. Python 3.8, 3.10, and 3.12 are tested for various releases. This can be controlled within the strategy matrix of the workflow. The Python versions should correspond to those used in the supported OS distributions for a particular release. The source for the workflow can be found here . It is managed centrally and imported into all downstream branches.","title":"Tox"},{"location":"usage/source-code-ci/#tag-release","text":"Software that depends on the source code repositories that the Release Train manages, typically will use a tag to identify the particular release to download and use. Therefore, to automate the process this workflow will generate a new tag for the latest commit to stackhpc/** which in turn will publish a new release under the given repository's Releases page. Warning The tag format currently used by this workflow is stackhpc/a.b.c.d which is NOT SemVer compliant and cannot be used in certain circumstances such as within Helm Charts . However, for the needs of the Release Train this is adequate.","title":"Tag &amp; Release"},{"location":"usage/source-code-ci/#upstream-sync","text":"Since many of our repositories are forked from OpenStack we need to ensure that we remain in sync with upstream in order to prevent situations where we deviate which could make any future attempts more difficult than it should be. Therefore, this workflow will periodically check if our stackhpc/** are behind their OpenStack counterparts. If so the workflow will copy the OpenStack branch into the repository and then make a pull request off of this copy ready to merge pending review by the relevant codeowner. Since the workflow uses the Github REST API it will still be able to open a PR even if it would result in a merge conflict allowing the relevant codeowner to make the necessary changes to resolve such a conflict. The workflow can be triggered manually within the Actions tab of a repository in addition to being scheduled to automatically. Currently is scheduled to run once a week on Monday at 09:15AM BST . This can be changed in the workflow template within the .github repository . As the workflow scheduled it must be located within the default branch such as main or master for Github to register it. 'on' : schedule : - cron : '15 8 * * 1' workflow_dispatch :","title":"Upstream Sync"},{"location":"usage/source-code-ci/#lint-collection","text":"Ansible collections are linted using Ansible lint , and sanity checked using the ansible-test sanity command. These checks run against pull requests to Ansible collection repositories and use a matrix to test multiple versions of Ansible. The tested versions of Ansible should generally correspond to those used by supported versions of Kayobe and Kolla.","title":"Lint Collection"},{"location":"usage/source-code-ci/#publish-collectionrole","text":"These two workflow automate the publication of a Ansible Collection or Role to Ansible Galaxy. They can triggered either manually or whenever a new tag is pushed in the form v[0-9]+.[0-9]+.[0-9]+ for a collection or v?[0-9]+.[0-9]+.[0-9]+ for a role.","title":"Publish Collection/Role"},{"location":"usage/source-code-ci/#synchronise-repositories-playbook","text":"Whilst the workflows can be imported manually this is NOT the intended approach, rather we will use an Ansible Playbook responsible for identifying repositories that are out of sync or missing the required files and perform pull requests to bring them inline. The playbook and role called source-repo-sync is located within this repository and will run inside of a Github Workflow, whenever a change is made to the configuration or variables. The motivation behind this is to have single-source of truth and free up developer time as creating dozens of branches and pull requests can be quite monotonous.","title":"Synchronise Repositories Playbook"},{"location":"usage/source-code-ci/#making-modifications-to-the-playbook","text":"Source Repositories Vars default_releases : list of OpenStack release series currently supported by StackHPC and used within the workflows openstack_workflows : dictionary of OpenStack specific workflows as mentioned above default_branch_only: list of workflows that will only exist on the default branch (master/main) elsewhere: list of workflows that will be placed on other branches such as stackhpc/xena see default_releases ansible_workflows : dictionary that contains either the type of Ansible collection or role collection: list of workflows specific to Ansible collections role: list of workflows specific to Ansible roles community_files : dictionary of templates for various community files. Each community file can have multiple templates are can be written as multiline strings. source_repositories : dictionary of repositories targeted by the Ansible playbook repository_name: points to repository, must match the name of the repository exactly. repository_type: used to distinguish if the repository is OpenStack, Ansible or branchless, defaults to OpenStack if not provided workflows: can either use openstack_workflows , ansible_workflows or provide a custom set of workflows, see ** below! ** --- default_releases : - xena - wallaby - victoria openstack_workflows : default_branch_only : - upstream-sync elsewhere : - tox - tag-and-release ansible_workflows : collection : - publish_collection role : - publish_role community_files : codeowners : kayobe_codeowners : | * @stackhpc/kayobe ansible_codeowners : | * @stackhpc/ansible source_repositories : kolla : community_files : - codeowners : content : '{{ community_files.codeowners.kayobe_codeowners }}' dest : '.github/CODEOWNERS' barbican : ignored_releases : - victoria - xena stackhpc-inspector-plugins : repository_type : 'branchless' workflows : '{{ openstack_workflows.elsewhere }}' ansible-role-os-networks : repository_type : 'ansible' workflows : '{{ ansible_workflows.role }}' ansible-collection-cephadm : repository_type : 'ansible' workflows : '{{ ansible_workflows.collection }}' In this subsection we shall explore how to make various modifications to the playbook. Please review the Source Repositories Vars for a description of the variables found within source-repositories vars file. Note Modifications can be made either by clone the stackhpc-release-train repository locally or using the Github Dev Editor by pressing . key within the repository.","title":"Making modifications to the playbook"},{"location":"usage/source-code-ci/#add-new-repository","text":"To add new repositories to be handled by this playbook you can edit source-repositories . Identify the source_repositories dictionary and insert your new repository. For example the below code snippet will add neutron to the source repo sync all default workflows and community files. Also all release series will be ignored except yoga . source_repositories : neutron : ignored_releases : - xena - wallaby - victoria community_files : - codeowners : content : \"{{ community_files.codeowners.openstack }}\" dest : \".github/CODEOWNERS\" Note Please refer to Making modifications to the playbook for description of changes that can be made.","title":"Add new repository"},{"location":"usage/source-code-ci/#changing-the-release-series","text":"To change the release series for all OpenStack repositories this can be achived by editing the default_releases variable. For example if you wanted to remove victoria and add support for yoga. Once this change has been merged into the main branch it shall perform a series of pull requests updating the workflows across all listed repositories. ansible/inventory/group_vars/all/source-repositories --- default_releases : - xena - wallaby - victoria --- default_releases : - yoga - xena - wallaby","title":"Changing the release series"},{"location":"usage/source-code-ci/#adding-new-workflows","text":"It is more involved to add additional workflows for distribution across the repositories. Add the workflow to the .github repository as one that is reusable. Add the workflow caller as Jinja template in the folder ansible/roles/source-repo-sync/templates such as deploy.jinja Update ansible/inventory/group_vars/all/source-repositories either by changing the default OpenStack or Ansible dict, which would propagate to all repositories of that type or updating only the repositories workflows dict directly. Use the name of the jinja template. See below. ansible/inventory/group_vars/all/source-repositories If you wish to update all repositories: openstack_workflows : default_branch_only : - deploy - upstream-sync elsewhere : - tox - tag-and-release If you want the new additional workflow to only be included in specific repository: source_repositories : kolla : workflows : additional_workflows : - deploy","title":"Adding new workflows"},{"location":"usage/source-code-ci/#remove-or-omit-workflows","text":"To remove a workflow from being deployed or updated across all repositories simply remove it from either openstack_workflows or ansible_workflows dict. If you would like to remove a workflow from a specific repository either because it is not necessary or you would prefer to manage that workflow within the repository itself then you can use ignore_workflows dict within the target repository. Note however, this will not create a pull request to remove it from the repositories if it has already been deployed. ansible/inventory/group_vars/all/source-repositories If you wish to remove a workflow from being deployed or updated for all repositories: openstack_workflows : default_branch_only : - upstream-sync elsewhere : - tag-and-release If you wish to remove a workflow from being deployed or updated to a specific repository: source_repositories : kolla : workflows : ignored_workflows : elsewhere : - tox","title":"Remove or omit workflows"},{"location":"usage/source-code-ci/#managing-community-files","text":"Github provides support for many community files that can be used throughout the release train repositories. Currently we support CODEOWNERS files which will automatically assign a team or team member to a pull request that relates to code that they own. All community files are stored in the community_files dictionary whereby the key is the name of file and then the value is a list of templates stored as multiline strings. To add a new community file or template simply expand the dictionary where appropriate and ensure that the relevant repositories are updated within the source_repositories dictionary. source_repositories : kolla : community_files : - codeowners : content : '{{ community_files.codeowners.kayobe_codeowners }}' dest : '.github/CODEOWNERS'","title":"Managing Community Files"},{"location":"usage/source-code-ci/#additional-notes","text":"All commits and pull requests are authored by the stackhpc-ci bot. The bots personal access token has been added to the secrets of this repository. If a branch or pull request originating from the source repositories playbook is not closed or deleted, prior to running further additonal changes then the bot will automatically delete the branch and pull request making room for the new changes. The source repositories playbook will NOT delete unmanaged files or branches. For example when we drop support for stackhpc/victoria the playbook will NOT delete the workflows from that branch. Due to the extensive use of the Github API the playbook will pause after every pull request for 10 seconds.","title":"Additional Notes"}]}